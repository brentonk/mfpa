[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Foundations of Political Analysis",
    "section": "",
    "text": "Preface\nThese are the course notes for Prof. Brenton Kenkel’s course PSCI 8350 at Vanderbilt University. The course covers the bare essential mathematical techniques needed for applied work in statistics and formal theory in political science.\nThis book is written in Quarto and published on GitHub Pages. You can find the Quarto source files on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "logic.html",
    "href": "logic.html",
    "title": "1  Formal Logic and Proofs",
    "section": "",
    "text": "1.1 Sentential logic and truth tables\nWhen you think about math, you probably think about numbers. The majority of the math classes you’ve taken in your life have been about how to manipulate numbers. But don’t panic if you’re the kind of person who grimaces a bit when you have to calculate a tip by hand — your success as a political scientist will have nothing to do with your ability to add or multiply numbers in your head. That’s what we have computers for.\nIn this course, we will approach math from a different angle. Our goal will be to make statements that are provably true. In essence, a statement is provably true if there is a logical defense against any possible objection to the statement. Anyone who follows the rules of deductive logic — the ones we will work through in this first unit of the course — must agree that the statement is true.\nPlenty of things are true in the ordinary sense of the word, yet are not provably true in the mathematical sense. For example, it is true that Joe Biden won the 2020 presidential election. However, the truth of this statement is established through empirical observation, not through logical deduction alone. In other words, at least some defenses against objections to this statement rely on findings of fact. If you say “I don’t think Joe Biden won the 2020 presidential election because I don’t think Joe Biden exists”, I need to convince you that Joe Biden exists, which isn’t a matter of logic alone.\nOn the other hand, the following statement is provably true: “If (a) only one person can win a presidential election, and (b) Joe Biden won the 2020 presidential election, and (c) Joe Biden is not Donald Trump, then (d) Donald Trump did not win the 2020 presidential election.” Once you accept the premises (a), (b), and (c), you have no choice but to reach the conclusion (d). Whether each premise is true is the kind of empirical question that logic alone cannot answer. But logic does tell us that if all these premises are true, then the conclusion (d) has to follow.\nIf you came to graduate school to study politics empirically, at this point you might be wondering why you should care about proving statements in the realm of pure logic. Here’s why I think you should care.\nSentential logic is a set of rules for working with sentences. In this context, a sentence is a statement that is either true or false (it must be one, and it cannot be both). Here are some examples of sentences:\nEach of these is a “sentence”, logically speaking, in that it is true or false. But there are important distinctions among them.\nIn one sense, our goal with formal logic will be to take compound sentences and sort them into these three categories: those that must be true, those that must be false, and those that are indeterminate, whose truth value cannot be deduced by logic alone.\nYou might think that the sentences that must be true — the tautologies — are inherently uninteresting. Certainly no one is going to be surprised that either the tariffs caused the stock market to crash or they didn’t. And yet there are indeed statements that are tautological yet nonobvious on first glance. For example, I was surprised to learn in graduate school that there is no non-dictatorial system for elections with more than two candidates that eliminates incentives for strategic voting. But this statement is indeed a tautology in the formal-logic sense of the word. (If you don’t think the truth of this statement is nonobvious, just try arguing on the Internet with people who think ranked-choice voting will fix elections!)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Formal Logic and Proofs</span>"
    ]
  },
  {
    "objectID": "logic.html#sentential-logic-and-truth-tables",
    "href": "logic.html#sentential-logic-and-truth-tables",
    "title": "1  Formal Logic and Proofs",
    "section": "",
    "text": "“The ‘Liberation Day’ tariffs caused the stock market to crash.”\n“The ‘Liberation Day’ tariffs caused the stock market to crash, or the ‘Liberation Day’ tariffs did not cause the stock market to crash.”\n“The ‘Liberation Day’ tariffs caused the stock market to crash, and the ‘Liberation Day’ tariffs did not cause the stock market to crash.”\n\n\n\nFormal logic by itself cannot help us figure out whether sentence #1 is true or false. That is a matter for empirical study.\nBy contrast, we don’t need to consult the real world to know that sentence #2 is true. We can deduce on the basis of logic alone that sentence #2 is true. We call a compound sentence like this, which must be true regardless of the underlying truth values of the sentences from which it is constructed, a tautology.\nFinally, we also don’t need to consult the real world to know that sentence #3 is false. There is no way for it to be the case that the tariffs both did and did not cause the stock market to crash.\n\n\n\n\n1.1.1 Constructing compound sentences\nIn this chapter, we will use the letters \\(P\\), \\(Q\\), and \\(R\\) to denote sentences. Whenever you see one of these letters, just think: “a thing that can be exactly one of ‘true’ or ‘false’”. For example, \\(P\\) might be the statement “The ‘Liberation Day’ tariffs caused the stock market to crash.”\nWe will build compound sentences using three operators. The first is the not operator, denoted by \\(\\neg P\\). The rules of the “not” operator are simple: \\(\\neg P\\) is true whenever \\(P\\) is false, and \\(\\neg P\\) is false whenever \\(P\\) is true.\nThe second basic operator is and, which connects two sentences, denoted \\(P \\land Q\\). An “and” statement is true whenever both underlying statements are true, and false otherwise.\nThe final basic operator is or, which again connects two sentences, denoted \\(P \\lor Q\\). An “or” statement is true whenever at least one of the underlying statements is true. It is only false when both of the underlying statements are false. In other words, the logical “or” is an “inclusive or” (allowing for the possibility that both are true), not an “exclusive or”.\nAbove we saw the compound sentence “The ‘Liberation Day’ tariffs caused the stock market to crash, or the ‘Liberation Day’ tariffs did not cause the stock market to crash.” If we take \\(P\\) to be the sentence “The ‘Liberation Day’ tariffs caused the stock market to crash”, then the formal statement of our compound sentence is \\(P \\lor \\neg P\\). We want to show that the compound sentence \\(P \\lor \\neg P\\) is a tautology; i.e., the compound sentence is true regardless of whether \\(P\\) is true or false.\nWe will use a truth table to identify the conditions under which a compound sentence is true. To build a truth table for a compound sentence like \\(P \\lor \\neg P\\), the first thing we do is identify the underlying sentences it is built from. In the case of \\(P \\lor \\neg P\\), there’s only one underlying sentence, namely \\(P\\). We begin to write the truth table by enumerating all combinations of truth values of the underlying sentences.\n\n\n\n\\(P\\)\n\n\n\n\ntrue\n\n\nfalse\n\n\n\nNext, we add columns to build up to the compound sentence that we are trying to evaluate. In the example here, we are trying to get from \\(P\\) to \\(P \\lor \\neg P\\). The compound sentence is built from \\(P\\) and \\(\\neg P\\). So we’ll start with \\(P\\), evaluate \\(\\neg P\\), and finally evaluate \\(P \\lor \\neg P\\).\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\lor \\neg P\\)\n\n\n\n\ntrue\n\n\n\n\nfalse\n\n\n\n\n\nWe then fill the columns by moving across the truth table from left to right. We know that \\(\\neg P\\) is false whenever \\(P\\) is true, and vice versa, which lets us fill out the second column.\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\lor \\neg P\\)\n\n\n\n\ntrue\nfalse\n\n\n\nfalse\ntrue\n\n\n\n\nFinally, we know that \\(P \\lor \\neg P\\) is true whenever at least one of \\(P\\) or \\(\\neg P\\) is true, which lets us fill out the third column.\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\lor \\neg P\\)\n\n\n\n\ntrue\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\n\n\n\nWe now see that the compound sentence \\(P \\lor \\neg P\\) is a tautology, as it is always true, regardless of whether the sentence it is built from is true or false.\n\n\n\n\n\n\nApproaching exercises\n\n\n\nTry the exercises yourself, ideally by writing them on paper or in a tablet, before looking at the answers. You learn the most by trying on your own and then checking. (This is exactly how I still do it when I’m working through unfamiliar technical material!)\n\n\n\nExercise 1.1 Use a truth table to show that \\(\\neg (P \\land \\neg P)\\) is a tautology.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nStart by setting up the table with each combination of possible values of the underlying sentences (in this case, just \\(P\\)), then with each sequential step you need to build the final compound sentence.\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\land \\neg P\\)\n\\(\\neg (P \\land \\neg P)\\)\n\n\n\n\ntrue\n\n\n\n\n\n; false\n\n\n\n\n\n\nThen fill each column across, using the information from the left.\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\land \\neg P\\)\n\\(\\neg (P \\land \\neg P)\\)\n\n\n\n\ntrue\nfalse\n\n\n\n\nfalse\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\land \\neg P\\)\n\\(\\neg (P \\land \\neg P)\\)\n\n\n\n\ntrue\nfalse\nfalse\n\n\n\nfalse\ntrue\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(\\neg P\\)\n\\(P \\land \\neg P\\)\n\\(\\neg (P \\land \\neg P)\\)\n\n\n\n\ntrue\nfalse\nfalse\ntrue\n\n\nfalse\ntrue\nfalse\ntrue\n\n\n\n\n\n\n\nThe more simple sentences that your compound sentence is built from, the more rows the truth table will end up having. For example, let’s build a truth table for the compound sentence \\(P \\lor \\neg (P \\land Q)\\). The truth table will now have four rows, one for each combination of true/false for \\(P\\) and \\(Q\\).\n\n\n\n\\(P\\)\n\\(Q\\)\n\n\n\n\ntrue\ntrue\n\n\ntrue\nfalse\n\n\nfalse\ntrue\n\n\nfalse\nfalse\n\n\n\nThen we break down the compound sentence and fill out the rest of the truth table the same way as before.\n\nColumn setupCompleted table\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\land Q\\)\n\\(\\neg (P \\land Q)\\)\n\\(P \\lor \\neg (P \\land Q)\\)\n\n\n\n\ntrue\ntrue\n\n\n\n\n\ntrue\nfalse\n\n\n\n\n\nfalse\ntrue\n\n\n\n\n\nfalse\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\land Q\\)\n\\(\\neg (P \\land Q)\\)\n\\(P \\lor \\neg (P \\land Q)\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\ntrue\n\n\ntrue\nfalse\nfalse\ntrue\ntrue\n\n\nfalse\ntrue\nfalse\ntrue\ntrue\n\n\nfalse\nfalse\nfalse\ntrue\ntrue\n\n\n\n\n\n\n\nExercise 1.2 Use a truth table to show that \\((P \\lor Q) \\lor (\\neg P \\land \\neg Q)\\) is a tautology.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSet up the rows:\n\n\n\n\\(P\\)\n\\(Q\\)\n\n\n\n\ntrue\ntrue\n\n\ntrue\nfalse\n\n\nfalse\ntrue\n\n\nfalse\nfalse\n\n\n\nSet up the columns:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\lor Q\\)\n\\(\\neg P\\)\n\\(\\neg Q\\)\n\\(\\neg P \\land \\neg Q\\)\n\\((P \\lor Q) \\lor (\\neg P \\land \\neg Q)\\)\n\n\n\n\ntrue\ntrue\n\n\n\n\n\n\n\ntrue\nfalse\n\n\n\n\n\n\n\nfalse\ntrue\n\n\n\n\n\n\n\nfalse\nfalse\n\n\n\n\n\n\n\n\nFill out the table:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\lor Q\\)\n\\(\\neg P\\)\n\\(\\neg Q\\)\n\\(\\neg P \\land \\neg Q\\)\n\\((P \\lor Q) \\lor (\\neg P \\land \\neg Q)\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\ntrue\n\n\ntrue\nfalse\ntrue\nfalse\ntrue\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\ntrue\nfalse\nfalse\ntrue\n\n\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\nIt’s reasonably simple to double-check your work in a truth table using R. (Or any programming language — they’re all good at this sort of binary logic exercise.) In R, to set up the rows of a truth table, you can use expand.grid() to enumerate all possible true-false combinations.\n\ngrid &lt;- expand.grid(Q = c(TRUE, FALSE), P = c(TRUE, FALSE))\nprint(grid)\n\n      Q     P\n1  TRUE  TRUE\n2 FALSE  TRUE\n3  TRUE FALSE\n4 FALSE FALSE\n\n\n\n\nI put Q first in the R code so that the row order will be the same as in the tables I made by hand above.\nIn R, ! means “not”, & means “and”, and | means “or”. We can use these operators to look at the columns of our truth table.\n\nP &lt;- grid$P\nQ &lt;- grid$Q\ncbind(P, Q, P | !(P & Q))\n\n         P     Q     \n[1,]  TRUE  TRUE TRUE\n[2,]  TRUE FALSE TRUE\n[3,] FALSE  TRUE TRUE\n[4,] FALSE FALSE TRUE\n\n\nYou can also use the all() function to quickly check whether a statement is a tautology.\n\nall(P | !(P & Q)) # is a tautology\n\n[1] TRUE\n\nall(P & Q) # not a tautology\n\n[1] FALSE\n\n\n\nExercise 1.3 Using R, confirm that the compound sentence \\[[(P \\lor Q) \\lor (\\neg R \\lor \\neg S)] \\lor [(\\neg P \\land \\neg Q) \\land (R \\land S)]\\] is a tautology.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ngrid &lt;- expand.grid(\n  P = c(TRUE, FALSE),\n  Q = c(TRUE, FALSE),\n  R = c(TRUE, FALSE),\n  S = c(TRUE, FALSE)\n)\nP &lt;- grid$P\nQ &lt;- grid$Q\nR &lt;- grid$R\nS &lt;- grid$S\n\n# Break sentence into two parts to keep track of things easier\npart_1 &lt;- (P | Q) | (!R | !S)\npart_2 &lt;- (!P & !Q) & (R & S)\n\n# Confirm tautology\nall(part_1 | part_2)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n1.1.2 “If”, “only if”, and “if and only if”\nIf-then statements work a bit differently in formal logic than in ordinary language. If I told you, “Scream if you see a bear!” and then you screamed, I would run away, having inferred that you saw a bear. However, I didn’t tell you “Don’t scream if you didn’t see a bear.” So in terms of pure logic, there would be no problem with you screaming even if there weren’t a bear — the only way to contradict the command would be to fail to scream when you actually did see a bear.\nIn the realm of formal logic and mathematics, we will be totally pedantic about our if-then statements. If I say “if \\(P\\), then \\(Q\\)”, the only way to falsify my statement is to show me that \\(P\\) is true and \\(Q\\) is false. If \\(P\\) is false, then the statement is true regardless of the truth of \\(Q\\).\n\nDefinition 1.1 The conditional statement “\\(P\\) implies \\(Q\\)” or “if \\(P\\), then \\(Q\\)”, written \\(P \\to Q\\), is logically equivalent to \\(\\neg P \\lor Q\\).\n\nWhen \\(P\\) is known to be false, we say that the statement \\(P \\to Q\\) is vacuously true. Here are some vacuously true statements:\n\nIf 0 = 1, then the moon is made of cheese.\nIf 4 is a prime number, then an asteroid will hit Earth on February 30, 2026.\nIf there is a finite game with no Nash equilibrium, then Professor Brad Smith is handsome and intelligent.\n\nWhile it’s fun to think about vacuous truths, conditional statements are most useful to us when the premise is true. Think about a conditional statement \\(P \\to Q\\) that we know to be true. As an uncontroversial example, let \\(P\\) be “today is Wednesday” and \\(Q\\) be “tomorrow is Thursday”, so that \\(P \\to Q\\) means “if today is Wednesday, then tomorrow is Thursday.” Imagine then we also know that \\(P\\) is true. As it happens, at the time I am writing this paragraph, it is indeed Wednesday. Given that \\(P\\) implies \\(Q\\), and that \\(P\\) is true, it seems logical for me to deduce that \\(Q\\) is true as well. Indeed, this deduction is logical — the name for this type of inference is modus ponens.\nYou don’t have to take the validity of modus ponens on faith. We can translate the rule into a compound sentence, and then we can prove that it is a tautology. In words, the rule is “If \\(P\\) implies \\(Q\\) and \\(P\\) is true, then \\(Q\\) is true.” Equivalently, we could say “If \\((P \\to Q) \\land P\\), then \\(Q\\).” Finally, we come to the most compact (though not the easiest to parse!) statement in Theorem 1.1.\n\nTheorem 1.1 (Modus ponens) \\([(P \\to Q) \\land P] \\to Q\\).\n\n\nProof. We will use a truth table to show that the statement is a tautology.\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\to Q\\)\n\\((P \\to Q) \\land P\\)\n\\([(P \\to Q) \\land P] \\to Q\\)\n\n\n\n\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\ntrue\nfalse\nfalse\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\nfalse\ntrue\n\n\nfalse\nfalse\ntrue\nfalse\ntrue\n\n\n\n\n\n\n\n\n\n\nNames of mathematical results\n\n\n\nThroughout these notes, you will see mathematical results labeled Theorem, Proposition, Lemma, and Corollary. Anything with one of these names is a formal statement that is provably true, and will usually be accompanied by a proof. None of these types of results is more or less true than the others — the different labels are just to help readers decode the context and importance of each type of result.\n\nTheorem: Reserved for results that are especially big, important, fundamental, general, etc. For example, Theorem 1.1 is a theorem because it is the foundation of logical deduction.\nProposition: A bread-and-butter result. Useful and important enough to care about on its own, but not earth-shaking enough to be a theorem.\nLemma: A result that we don’t necessarily care about on its own, but is a useful building block toward one or more propositions or theorems.\nCorollary: A result that follows almost immediately from some earlier lemma(s), proposition(s), or theorem(s). My general heuristic for calling something a corollary is that it can be proved in two sentences or less, and the proof requires invoking an earlier lemma, proposition, or theorem.\n\n\n\nYou can think of \\(P \\to Q\\) as a statement of a sufficient condition: if \\(P\\) is true, then \\(Q\\) must be true, hence \\(P\\) is “sufficient” to ensure that \\(Q\\) holds. However, this statement says nothing about whether \\(P\\) is a necessary condition for \\(Q\\) — something that must be true in order for \\(Q\\) to be true. Think about the statement “If Marie is a member of the House of Representatives, then Marie is a politician.” Being a member of the House is sufficient to be called a politician, but it is not necessary. We would still call Marie a politician if she were the president, a senator, a yet-unelected candidate for the House, or even the state comptroller.\nIn the same way that the statement \\(P \\to Q\\) gives us a sufficient condition for \\(Q\\), it also gives us a necessary condition for \\(P\\). Remember that the conditional statement \\(P \\to Q\\) turns out to be false when \\(P\\) is true yet \\(Q\\) is false. Hence, an alternative way to verbalize \\(P \\to Q\\) is “\\(P\\) is true only if \\(Q\\) is true,” meaning \\(Q\\) is a necessary — though perhaps insufficient — condition for \\(P\\). Returning to the example above, we could say “Marie is a member of the House of Representatives only if she is a politician.” Being a politician doesn’t necessarily mean she’s a House member, but she certainly cannot be a House member unless she is a politician.\nWe’ve just seen that a sufficient condition (if \\(P\\), then \\(Q\\)) can be translated into a necessary condition (\\(P\\) only if \\(Q\\)). This observation allows us to make the following type of logical deduction:\n\nPremise 1: If Marie is a member of the House of Representatives, then Marie is a politician. (\\(P \\to Q\\))\nPremise 2: Marie is not a politician. (\\(\\neg Q\\))\nConclusion: Marie is not a member of the House of Representatives. (\\(\\neg P\\))\n\nThe line of reasoning here is an example of the deductive rule called modus tollens. If we know that \\(P\\) implies \\(Q\\) and we know that \\(Q\\) is false, we must conclude that \\(P\\) is false as well.\n\nTheorem 1.2 (Modus tollens) \\([(P \\to Q) \\land \\neg Q] \\to \\neg P\\).\n\n\nProof. Try to prove this yourself using a truth table, following the same lines as the proof of Theorem 1.1 above. If you get stuck, check the answer below.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\to Q\\)\n\\(\\neg Q\\)\n\\((P \\to Q) \\land \\neg Q\\)\n\\(\\neg P\\)\n\\([(P \\to Q) \\land \\neg Q] \\to \\neg P\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\ntrue\n\n\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\nfalse\nfalse\ntrue\ntrue\n\n\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\n\nHere’s another way to think about this proof, somewhat anticipating our discussion of proof by contradiction below.\nThe only way for \\([(P \\to Q) \\land \\neg Q] \\to \\neg P\\) to be false would be for \\((P \\to Q) \\land \\neg Q\\) to be true while \\(\\neg P\\) is false (i.e., \\(P\\) is true). There are two paths for \\(P \\to Q\\) to be true: either it’s vacuously true because \\(P\\) is false, or it’s true because \\(P\\) is true and so is \\(Q\\).\nWhen we additionally assume \\(\\neg Q\\) is true, then \\(P \\to Q\\) can only be true vacuously, i.e., because \\(P\\) is false. In other words, whenever \\(P \\to Q\\) and \\(\\neg Q\\) are both true, it must be the case that \\(P\\) is false and thus \\(\\neg P\\) is true. This means there is no path for \\([(P \\to Q) \\land \\neg Q] \\to \\neg P\\) to be falsified, as anytime the premise \\((P \\to Q) \\land \\neg Q\\) is true, the conclusion \\(\\neg P\\) must be true too.\nTherefore, the statement of modus tollens is a tautology.\n\n\n\n\nThe statements \\(P\\) and \\(Q\\) are logically equivalent when \\(P \\to Q\\) and \\(Q \\to P\\). In this case, the truth values of the two statements are linked: either \\(P\\) and \\(Q\\) are both true, or else \\(P\\) and \\(Q\\) are both false. \\(P\\) is a necessary and sufficient condition for \\(Q\\), and \\(Q\\) is a necessary and sufficient condition for \\(P\\). In words, we may use “\\(P\\) if and only if \\(Q\\)” to describe this state of affairs.\nAs an example, take the “minimalist conception” of democracy posed by Przeworski (2024, 5): “A regime is democratic if and only if people are free to choose, including to remove, governments.” Let \\(P\\) be the statement “The regime is democratic” and \\(Q\\) be the statement “The regime’s people are free to choose, including to remove, governments.”\n\nThe statement \\(P \\to Q\\) means “The regime is democratic only if its people are free to choose, including to remove, governments” — the people’s ability to remove the government is a necessary condition for democracy.\nThe statement \\(Q \\to P\\) means “The regime is democratic if its people are free to choose, including to remove, governments” — the people’s ability to remove the government is a sufficient condition for democracy.\n\nLogical equivalence means \\(P\\) implies \\(Q\\) and vice versa, so we denote it with the biconditional \\(P \\leftrightarrow Q\\). The statement \\(P \\leftrightarrow Q\\) is true whenever the truth values of \\(P\\) and \\(Q\\) match, and false otherwise. If you’re skeptical, you can use a truth table to confirm that the statement \\((P \\to Q) \\land (Q \\to P)\\) is true exactly when the truth values of \\(P\\) and \\(Q\\) match.\n\nExercise 1.4 Use a truth table to confirm that the statement \\((P \\to Q) \\land (Q \\to P)\\) is true exactly when the truth values of \\(P\\) and \\(Q\\) match.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\to Q\\)\n\\(Q \\to P\\)\n\\((P \\to Q) \\land (Q \\to P)\\)\n\n\n\n\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\ntrue\nfalse\nfalse\ntrue\nfalse\n\n\nfalse\ntrue\ntrue\nfalse\nfalse\n\n\nfalse\nfalse\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\nAn important logical equivalence, closely related to modus tollens (Theorem 1.2), is the contrapositive: \\(P \\to Q\\) is logically equivalent to \\(\\neg Q \\to \\neg P\\). The contrapositive is more useful in practice than you might expect. When you want to prove that \\(P \\to Q\\), sometimes it’s easier to start with \\(\\neg Q\\) and show that \\(\\neg P\\) must hold than to start with \\(P\\) and show that \\(Q\\) must hold. Even more radically, as we’ll see in Section 1.2.3 below, when you want to prove that \\(P\\) is true, sometimes it’s easiest to start with \\(\\neg P\\) and show that you end up with something known to be false.\n\nExercise 1.5 Use a truth table to confirm that \\(P \\to Q\\) is logically equivalent to \\(\\neg Q \\to \\neg P\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\to Q\\)\n\\(\\neg P\\)\n\\(\\neg Q\\)\n\\(\\neg Q \\to \\neg P\\)\n\\((P \\to Q) \\leftrightarrow (\\neg Q \\to \\neg P)\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\nfalse\ntrue\ntrue\n\n\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\n\n\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\n\n\n1.1.3 De Morgan’s laws\nWe have to be careful when using negation in combination with the “and” and “or” operators. This is probably easier when we’re working in words than when we’re working with notation, but we need caution either way. For example, let \\(F\\) be the statement “Finland is a democracy” and \\(R\\) be the statement “Russia is a democracy”. There are three ways for the compound statement \\(F \\land R\\) to be false:\n\n\\(F \\land \\neg R\\): Finland is a democracy, but Russia is not.\n\\(\\neg F \\land R\\): Finland is not a democracy, but Russia is.\n\\(\\neg F \\land \\neg R\\): Finland and Russia both are not democracies.\n\nIn other words, if \\(F \\land R\\) is false, then we know at least one of \\(F\\) or \\(R\\) must be false — but without other information, we can’t say which one. In other other words, we can conclude from \\(\\neg (F \\land R)\\) that \\(\\neg F \\lor \\neg R\\). The negation of an “and” statement gives us an “or” statement.\nI’m highlighting this because I don’t want you to be tempted to treat the formal logic operators like addition and multiplication. In the realm of high-school algebra, you might remember that \\[-(f + r) = (-f) + (-r).\\] Yet in the realm of sentential logic, the statement \\(\\neg (F \\land R)\\) is not equivalent to \\(\\neg F \\land \\neg R\\).\nDe Morgan’s laws are a pair of logical equivalences that tell us exactly how to combine negation with “and” and “or.” Before stating them formally, here’s how I think about them informally.\n\nAn “and” statement is strong, since \\(F \\land R\\) means that both underlying statements are true. The negation of an “and” statement must then be weak, because \\(\\neg (F \\land R)\\) only means that at least one of the underlying statements is false.\nAn “or” statement is weak, since \\(F \\lor R\\) only means that at least one of the underlying statements is true. The negation of an “or” statement must then be strong, because \\(\\neg (F \\lor R)\\) means that both underlying statements are false.\nTherefore, the negation of an “and” statement must be an “or” statement, and vice versa.\n\n\nTheorem 1.3 (De Morgan’s laws) \\[\\begin{align*}\n\\neg (P \\land Q) &\\leftrightarrow (\\neg P \\lor \\neg Q) \\\\\n\\neg (P \\lor Q) &\\leftrightarrow (\\neg P \\land \\neg Q)\n\\end{align*}\\]\n\n\nProof. I will prove the first law with a truth table, then leave the second one to you as an exercise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\land Q\\)\n\\(\\neg (P \\land Q)\\)\n\\(\\neg P\\)\n\\(\\neg Q\\)\n\\(\\neg P \\lor \\neg Q\\)\n\\(\\neg (P \\land Q) \\leftrightarrow (\\neg P \\lor \\neg Q)\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\ntrue\n\n\ntrue\nfalse\nfalse\ntrue\nfalse\ntrue\ntrue\ntrue\n\n\nfalse\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\n\n\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\n\n\nProof of the second law\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(Q\\)\n\\(P \\lor Q\\)\n\\(\\neg (P \\lor Q)\\)\n\\(\\neg P\\)\n\\(\\neg Q\\)\n\\(\\neg P \\land \\neg Q\\)\n\\(\\neg (P \\lor Q) \\leftrightarrow (\\neg P \\land \\neg Q)\\)\n\n\n\n\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\ntrue\n\n\ntrue\nfalse\ntrue\nfalse\nfalse\ntrue\nfalse\ntrue\n\n\nfalse\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\ntrue\n\n\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Formal Logic and Proofs</span>"
    ]
  },
  {
    "objectID": "logic.html#methods-of-proof",
    "href": "logic.html#methods-of-proof",
    "title": "1  Formal Logic and Proofs",
    "section": "1.2 Methods of proof",
    "text": "1.2 Methods of proof\nThe vast majority of proofs you’ll read — and write — will not be in the form of a truth table. The number of rows in the truth table grows exponentially with the number of sentences, making it unwieldy to use truth tables to prove complex claims. For example, it is hard to imagine using a truth table to prove the median voter theorem (Black 1948), an important but relatively simple result as formal theories of politics go.\n\n\n“Grows exponentially” is a phrase with a precise meaning that people often use imprecisely. Here I mean it in the literal sense, as a compound statement built from \\(N\\) underlying sentences requires a truth table with \\(2^N\\) rows. In words, each additional sentence doubles the number of rows in the truth table.\nA proof is written in ordinary language, just with a bit more attention to precision than you might use in other ordinary writing. The goal is to convince the reader of the truth of whatever claim you have made, following the basic rules of logical inference we have established here. In essence, you need to show the reader why any objection to your claim will ultimately fail.\nUse mathematical notation sparingly in proofs. Only use notation when it’s necessary for precision or brevity. In almost all settings, it is much better to write “Let \\(n\\) be an odd number” than to write “Let \\(n \\in \\{2m - 1 \\mid m \\in \\mathbb{N}\\}\\).”\n\n\n\n\n\n\nReading proofs\n\n\n\nI’m deliberately more verbose in the proofs here than in the proofs you’d see in a published paper, or even in most textbooks. The structure of arguments in those venues tends to be the same as the one here, though — they just have a bit less verbiage to hold the reader’s hand.\nIn proofs in (other) textbooks and academic articles, you’ll often see phrases like “It is obvious that…” or “It is straightforward to show that…”. Students — and professors! — understandably find these statements frustrating or alienating, because whatever comes next often does not seem obvious or straightforward at all. But really, “It is obvious that…” is shorthand for:\n\nThe statement that comes next can be proved using mathematical methods that a reader of this document would typically have extensive practice with. (For example, in a textbook about calculus, such methods would include arithmetic and algebra. In a paper in the Journal of Economic Theory, they would include calculus, differential equations, real analysis, and topology.) There aren’t any new tricks or novel arguments to reach this step, just chugging through calculations. You are welcome to do those calculations yourself to check my work, though I expect you won’t find anything particularly edifying along the way. Because space is limited in this book or journal article, I haven’t included all the gory details myself.\n\nWhat if the proof you’re reading says something is obvious or straightforward, but you can’t convince yourself it’s true? After getting out your legal pad and trying to work it out yourself, that’s when you should ask for help. I find myself in this situation often, and as of summer 2025 I’ve found that ChatGPT’s o4-mini-high model gives great explanations when I’m stuck on something mathematical. And throughout your careers here, you can always feel free to ask me or the other methods/formal theory faculty when you’re stuck on something!\nProofs conventionally end with “QED”, \\(\\blacksquare\\), or \\(\\square\\). As of summer 2025, the Quarto software I’m using to write these notes doesn’t insert any of these symbols automatically, so you’ll just have to take the bottom of the box that the proof lives in as my “QED” variant.\n\n\n\n1.2.1 Proving an “if” statement\nLet’s practice writing a plain language proof of a statement about formal logic. The statement I want to prove is called the law of transitivity: if we know that \\(P \\to Q\\) and that \\(Q \\to R\\), then we can conclude that \\(P \\to R\\).\n\nProposition 1.1 (Law of transitivity) Logical implication is transitive: \\([(P \\to Q) \\land (Q \\to R)] \\to (P \\to R)\\).\n\nWe could prove this using a truth table, and in fact the first Google hit for “transitivity of implication” does exactly that. But I want to walk you through how I’d write an ordinary language proof, step by step. I’ll put each step in a blockquote, with explanation thereafter.\n\nSuppose \\(P \\to Q\\) and \\(Q \\to R\\).\n\nThe most common way to kick off a proof of an if-then statement is to state that we will assume the premise (the “if”) is true. That might seem like a weird thing to do when the premise could very well be false. However, remember that in formal logic, the only way to falsify a conditional statement is to show that the conclusion might fail when the premise is true. If the premise is false, then the conditional is (vacuously) true. So to demonstrate the overall truth of the conditional statement, it’s enough for us to show that the conclusion must hold whenever the premise does.\nHaving assumed the premise, where do we go from here? We ultimately need to reach the conclusion that \\(P \\to R\\). To get there, we’re going to use the common trick of breaking the proof into cases. We know that either \\(P\\) is true or \\(P\\) is false (i.e., \\(P \\lor \\neg P\\) is a tautology). Let’s show that either one of these possibilities leads to the conclusion we want, namely that \\(P \\to R\\).\n\n\nAre you skeptical that this trick is logically valid? If so, don’t worry — you’ll prove its logical validity yourself on the problem set.\n\nThere are two cases to consider: when \\(P\\) is true, and when \\(P\\) is false. We will show that \\(P \\to R\\) holds in both cases.\n\nAs you might already know from trying to read proofs, it’s easy for a reader to get lost in the formal logic. So the line above is just an explicit signpost, attempting to signal: “We’re going to break the proof into mutually exhaustive cases, showing that each case leads to our desired conclusion, and thus the conclusion must hold.” It’s kind of like writing comments in your R code — not strictly necessary for the program to work, but very helpful for anyone who’s trying to follow it.\n\nFirst, suppose \\(P\\) is true. Because \\(P\\) and \\(P \\to Q\\) are both true, we conclude by modus ponens that \\(Q\\) is true.\n\nWe’ve started the analysis of the first case here. You can start to see why the cases trick is useful: having assumed initially that \\(P \\to Q\\), and now that \\(P\\) is true as well, we can proceed to \\(Q\\). Here we make that line of logic explicit.\nThere’s also a meta-lesson here about knowing your audience when you write a proof. I’m setting out this proof as an introduction for students without much background in reading or writing math-style proofs. Hence, I’m trying to be detailed and explicit, down to acknowledging that modus ponens is the logical rule I’m using to infer \\(Q\\) from the combination of \\(P \\to Q\\) and \\(P\\). In a proof for an article I were submitting to a journal, I would not bother to say I was using modus ponens; I would assume my readers were familiar with the basic rules of logical inference. That said, when you aren’t sure how to proceed, err on the side of giving details rather than skipping steps.\n\nThen, because \\(Q\\) and \\(Q \\to R\\), we conclude (again by modus ponens) that \\(R\\) is true. Because \\(P\\) and \\(R\\) are both true, the statement \\(P \\to R\\) is true as well.\n\nHere we keep following the line of logic, using our inference about \\(Q\\) in the last step to support an inference of \\(R\\) in this step. We then reach the conclusion we were looking for, namely that when we start with \\(P\\) (in addition to the assumptions of \\(P \\to Q\\) and \\(Q \\to R\\) that we made at the outset of the proof) we get to \\(P \\to R\\).\n\nSecond, suppose \\(P\\) is false. Then it is vacuously true that \\(P \\to R\\).\n\nThe second case is simpler than the last one. (I personally try to arrange my proofs so that the trickier case comes first, under the assumption that readers’ attention is ever-waning, but it’s really a judgment call.) And again we’ve written it up to try to be friendly to the reader. We could have simply written, “Second, \\(\\neg P\\) implies \\(P \\to R\\),” but with just a few more words we make it clear precisely why we’re drawing this conclusion.\n\nAltogether, we have shown that \\(P \\to R\\) whether \\(P\\) is true or false. We conclude that \\(P \\to R\\).\n\nThis is one last little bit of logical signposting, summing up the line of logic that has gotten us to the ultimate conclusion. This might be overkill when the proof is so short, but I’m including it by my own logic that it’s better to do a bit too much hand-holding than a bit too little.\nPutting all of this together to prove Proposition 1.1:\n\nProof. Suppose \\(P \\to Q\\) and \\(Q \\to R\\). There are two cases to consider: when \\(P\\) is true, and when \\(P\\) is false. We will show that \\(P \\to R\\) holds in both cases.\nFirst, suppose \\(P\\) is true. Because \\(P\\) and \\(P \\to Q\\) are both true, we conclude by modus ponens that \\(Q\\) is true. Then, because \\(Q\\) and \\(Q \\to R\\), we conclude (again by modus ponens) that \\(R\\) is true. Because \\(P\\) and \\(R\\) are both true, the statement \\(P \\to R\\) is true as well.\nSecond, suppose \\(P\\) is false. Then it is vacuously true that \\(P \\to R\\).\nAltogether, we have shown that \\(P \\to R\\) whether \\(P\\) is true or false. We conclude that \\(P \\to R\\).\n\n\nExercise 1.6 Write a plain language proof of the modus tollens rule (Theorem 1.2).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour proof will of course differ from mine, but here’s how I approached it.\nSuppose that \\(P \\to Q\\) is true and that \\(Q\\) is false. We know from Exercise 1.5 that \\(P \\to Q\\) is logically equivalent to the contrapositive \\(\\neg Q \\to \\neg P\\). Then, because we know that \\(\\neg Q\\) and \\(\\neg Q \\to \\neg P\\) are both true, we infer by modus ponens that \\(\\neg P\\) is true. Consequently, we have proved that the conjunction of \\(P \\to Q\\) and \\(\\neg Q\\) implies \\(\\neg P\\).\n\n\n\n\n\nExercise 1.7 Assume \\(a\\) is an integer. Write a plain language proof of the claim that if \\(a^2\\) is even, then \\(a\\) is even. (An integer \\(a\\) is even if it is divisible by 2: there is some integer \\(n\\) such that \\(a = 2 \\times n\\).)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nI will prove the contrapositive. Assume \\(a\\) is not even, so it is not a factor of 2. Then \\(a^2 = a \\times a\\) also is not a factor of 2 and is not even. We have shown that if \\(a\\) is not even, then \\(a^2\\) is not even; this is equivalent to the claim that if \\(a^2\\) is even, then \\(a\\) is even.\n\n\n\n\n\n\n1.2.2 Proving an “if and only if” statement\nA claim of logical equivalence (\\(P \\leftrightarrow Q\\)) is a claim that two implications hold (\\(P \\to Q\\) and \\(Q \\to P\\)). So the most straightforward way to prove that \\(P\\) and \\(Q\\) are equivalent is to prove each implication individually.\nAs an example, let’s prove that the “and” operator is associative, meaning that \\((P \\land Q) \\land R\\) is logically equivalent to \\(P \\land (Q \\land R)\\). The associative property is convenient because, at a minimum, it lets us kill the parentheses and write \\(P \\land Q \\land R\\).\n\n\nBut be careful: you can’t necessarily change or drop parentheses when you’re using different operators. To wit, \\((P \\land Q) \\lor R\\) is not logically equivalent to \\(P \\land (Q \\lor R)\\).\n\nProposition 1.2 (Associativity of “and”) The “and” operator is associative: \\([(P \\land Q) \\land R] \\leftrightarrow [P \\land (Q \\land R)]\\).\n\n\nProof. We begin by proving that \\((P \\land Q) \\land R\\) implies \\(P \\land (Q \\land R)\\). Suppose it is true that \\((P \\land Q) \\land R\\). Then it must be the case that \\(P \\land Q\\) is true, as is \\(R\\). Because \\(P \\land Q\\) is true, we infer that \\(P\\) and \\(Q\\) are each true as well. Because \\(Q\\) and \\(R\\) are true, \\(Q \\land R\\) is true. Finally, because \\(P\\) and \\(Q \\land R\\) are each true, it is true that \\(P \\land (Q \\land R)\\).\nThe proof that \\(P \\land (Q \\land R)\\) implies \\((P \\land Q) \\land R\\) is similar. Suppose that \\(P \\land (Q \\land R)\\) is true. We infer that \\(P\\) is true and that \\(Q \\land R\\) is true, and from the latter we infer that \\(Q\\) and \\(R\\) are each true. The truth of \\(P\\) and \\(Q\\) gives us \\(P \\land Q\\), which combined with the truth of \\(R\\) gives us \\((P \\land Q) \\land R\\).\n\nOnce again, notice the signposting in the way the proof is written. You want to clearly delineate when you’re proving one direction versus when you’re proving the other. This is particularly important for longer proofs, where a reader might get lost about exactly which logical step you’re working through at any given point. You’ll also notice that the second paragraph explicitly states that the method of proof is similar to the first paragraph. This lets readers know that there’s nothing new going on here, so they can skim or skip if they understood the prior logic (or to read with skepticism if they didn’t buy the prior logic!).\nAnother common way to prove a claim like \\(P \\leftrightarrow Q\\) is to prove that \\(P \\to Q\\) and that \\(\\neg P \\to \\neg Q\\). This is equivalent to the last method of proof, as we know that the contrapositive \\(\\neg P \\to \\neg Q\\) is equivalent to \\(Q \\to P\\). Despite this equivalence in a formal logical sense, sometimes for purposes of writing the proof in plain English it’s easier to approach from this direction.\nThere’s one more way to prove a logical equivalence that’s more concise — but trickier — than the two methods I mentioned above. This third way is what I’ll call the chain of equivalences: to prove that \\(P \\leftrightarrow Q\\), prove that there is some third statement \\(R\\) for which \\(P \\leftrightarrow R\\) and \\(R \\leftrightarrow Q\\). Or add more steps in between if you need: prove that \\(P \\leftrightarrow R\\), that \\(R \\leftrightarrow S\\), that \\(S \\leftrightarrow T\\), and finally that \\(T \\leftrightarrow Q\\). The important thing to keep in mind is that every step along the way must be a full equivalence (“if and only if”), not a mere conditional (just “if” or just “only if”). Usually I end up taking the long way to prove logical equivalences, but it’s a nice treat when I can find a single chain of equivalence that works. For an example of a proof that works this way, see Section 1.2.4 below.\n\n\n1.2.3 Proof by contradiction\nAnother common proof technique, the proof by contradiction, is built around a reductio ad absurdum. We want to prove that some statement \\(P\\) is true. To do so, we show that if \\(P\\) isn’t true, then we end up coming to some absurd conclusion. We infer that \\(P\\) must be true.\nOne of my favorite (silly) proofs by contradiction is the proof that every counting number \\(n = 1, 2, \\ldots\\) is interesting.\n\nProposition 1.3 (not really a proposition) Every natural number \\(n = 1, 2, \\ldots\\) is interesting.\n\n\nProof. We will prove the claim by contradiction. Suppose the claim is false, so there is at least one natural number that is not interesting. By the well-ordering principle, this means there is a number \\(m\\) which is the smallest natural number that is not interesting. However, it is interesting that \\(m\\) is the smallest natural number that is not interesting. We have reached a contradiction, having shown that \\(m\\) is both uninteresting and interesting. Therefore, it must be the case that every natural number is interesting.\n\nA more tedious, but also more famous and important, proof by contradiction is the proof that \\(\\sqrt{2}\\) cannot be expressed as a fraction. In other words, \\(\\sqrt{2}\\) is an irrational number.\n\nProposition 1.4 There is no pair of integers \\(p\\) and \\(q\\) for which \\(\\sqrt{2} = \\frac{p}{q}\\).\n\n\nProof. We will prove the claim by contradiction. Suppose the claim is false, so there is an integer \\(p\\) and an integer \\(q \\neq 0\\) such that \\(\\sqrt{2} = \\frac{p}{q}\\).\nThe first thing we are going to do is remove all common factors of 2 from \\(p\\) and \\(q\\). In other words, we are going to reduce the fraction \\(\\frac{p}{q}\\) until the numerator is odd or the denominator is odd (or both). If \\(p\\) or \\(q\\) is odd, then let \\(a = p\\) and let \\(b = q\\). Otherwise, if \\(p\\) and \\(q\\) are both even, then divide both by 2. Continue this process until at least one of \\(p\\) or \\(q\\) is not divisible by 2, and let \\(a\\) and \\(b\\) be the results. Because we always divided the numerator and the denominator by the same factor, we end up with the same fraction: \\(\\frac{a}{b} = \\frac{p}{q} = \\sqrt{2}\\).\nBecause \\(\\frac{a}{b} = \\sqrt{2}\\), we have \\(\\frac{a^2}{b^2} = 2\\) and thus \\(a^2 = 2 b^2\\). Because \\(b^2\\) is an integer, this means \\(a^2\\) is divisible by 2 and thus is an even number, which in turn means \\(a\\) is an even number (see Exercise 1.7). Therefore, because we constructed \\(a\\) and \\(b\\) so that one of them at most could be even, \\(b\\) must not be an even number.\nBecause \\(a\\) is even and thus divisible by 2, it must be the case that \\(a^2\\) is divisible by 4. This in turn means that \\(a^2 / 2\\) is divisible by 2. But remember that \\(a^2 = 2 b^2\\) and thus \\(b^2 = a^2 / 2\\), so we have that \\(b^2\\) is even. This in turn implies that \\(b\\) must be an even number (again see Exercise 1.7).\nBy assuming that \\(\\sqrt{2}\\) is rational, we have come to the contradictory conclusion that there is an integer that is both even and not even. Therefore, \\(\\sqrt{2}\\) is not rational.\n\nAs we did with modus ponens (Theorem 1.1) and modus tollens (Theorem 1.2), we can use a truth table to establish the logical validity of proof by contradiction. First we need to state the logic behind it in the form of a compound sentence. We want to show that \\(P\\) is true. To do so, we show that if \\(P\\) is false, then we reach some conclusion that we know not to be true. I’m going to represent this with a special sentence \\(F\\), whose truth value is always false. (If you’re not comfortable with this, you could replace \\(F\\) with the negation of a tautology, such as \\(Q \\land \\neg Q\\).) The idea behind proof by contradiction is that if \\(\\neg P\\) implies \\(F\\), then we conclude \\(P\\) is true. Stated formally, the line of logic is \\((\\neg P \\to F) \\to P\\).\n\nTheorem 1.4 (Proof by contradiction) Letting \\(F\\) be a sentence that is always false, \\((\\neg P \\to F) \\to P\\).\n\n\nProof. We can prove the claim using a truth table:\n\n\n\n\n\n\n\n\n\n\n\\(P\\)\n\\(F\\)\n\\(\\neg P\\)\n\\(\\neg P \\to F\\)\n\\((\\neg P \\to F) \\to P\\)\n\n\n\n\ntrue\nfalse\nfalse\ntrue\ntrue\n\n\nfalse\nfalse\ntrue\nfalse\ntrue\n\n\n\nAlternatively, here’s a plain language proof of the claim. Suppose \\(\\neg P \\to F\\). Either this claim is vacuously true, or else \\(\\neg P \\land F\\). Since \\(\\neg P \\land F\\) cannot possibly be true, as \\(F\\) is false, the claim must be vacuously true. Vacuous truth of \\(\\neg P \\to F\\) means that \\(\\neg (\\neg P)\\) is true, which is equivalent to \\(P\\) being true. Therefore, \\(\\neg P \\to F\\) implies that \\(P\\) is true.\n\n\n\n1.2.4 Proof by induction\nEarlier we saw De Morgan’s laws (Theorem 1.3), which tell us that the negation of an “and” statement is an “or” statement, and vice versa. Intuitively, it seems like it should be true that for any number \\(n\\) of statements, \\(P_1, \\ldots, P_n\\), the negation \\(\\neg (P_1 \\land \\cdots \\land P_n)\\) is logically equivalent to \\(\\neg P_1 \\lor \\cdots \\lor \\neg P_n\\). De Morgan’s laws tell us that this holds in the special case \\(n = 2\\). But how can we show that it holds for a conjunction of \\(n = 3\\), or \\(n = 100\\), or \\(n = 10^{10^{10}}\\) statements as well?\nTo prove that De Morgan’s laws extend to a conjunction of any (finite) number of statements, we will write a proof by induction. This is a proof technique designed for the following situation:\n\nWe are dealing with a claim whose precise statement depends on a number \\(n\\). As shorthand to denote “a claim \\(Q\\) whose precise form depends on a number \\(n\\)”, we will write \\(Q(n)\\).\nIn the De Morgan’s law example here, the claim \\(Q(1)\\) is the trivial statement that \\(\\neg P_1 \\leftrightarrow \\neg P_1\\). \\(Q(2)\\) is the De Morgan’s law we already proved, \\(\\neg (P_1 \\land P_2) \\leftrightarrow (\\neg P_1 \\lor \\neg P_2)\\). \\(Q(3)\\) is \\(\\neg (P_1 \\land P_2 \\land P_3) \\leftrightarrow (\\neg P_1 \\lor \\neg P_2 \\lor \\neg P_3)\\). And so on.\nWe want to show that \\(Q(n)\\) is true for all numbers \\(n = 1, 2, 3, \\ldots\\), and so on infinitely.\nIn the De Morgan’s law example, we could definitely use a truth table to prove the statement for a reasonably small \\(n\\), like 3 or 4. But even once we get to \\(n = 10\\), we’re talking about a truth table with 1,024 rows. That seems like overkill to prove a claim that intuitively seems like it ought to be true.\n\nA proof by induction breaks the seemingly infinite task of proving \\(Q(n)\\) for all \\(n\\) into just two steps. First, in the base step, we prove that the claim \\(Q(n)\\) is true for \\(n = 1\\). In the example here, \\(Q(1)\\) is just the statement that \\(\\neg P_1 \\leftrightarrow \\neg P_1\\), which of course is true. That establishes the base step.\nSecond, in the induction step, we prove that for any number \\(k\\), if \\(Q(k)\\) is true, then \\(Q(k + 1)\\) is true. In combination, the base step and the induction step establish that every \\(Q(n)\\) must be true. The induction step proves that \\(Q(1) \\to Q(2)\\), and the base step proves that \\(Q(1)\\) is true. Hence, by modus ponens, \\(Q(2)\\) is true. The induction step proves that \\(Q(2) \\to Q(3)\\), so again by modus ponens, \\(Q(3)\\) is true. We can follow this chain of logic up to any \\(n\\) that we want.\nThe induction step is usually the trickier part of the proof. Let’s work through the induction step for our claim about De Morgan’s law.\n\nProof. We need to show that \\(Q(k)\\) implies \\(Q(k + 1)\\). To do that, as in a typical proof of a conditional statement, we will begin by assuming \\(Q(k)\\) is true, i.e., that \\[\\neg (P_1 \\land \\cdots \\land P_k) \\leftrightarrow (\\neg P_1 \\lor \\cdots \\lor \\neg P_k).\\] Our goal is to show that \\(Q(k + 1)\\) is true, i.e., that \\[\\neg (P_1 \\land \\cdots \\land P_k \\land P_{k+1}) \\leftrightarrow (\\neg P_1 \\lor \\cdots \\lor \\neg P_k \\lor \\neg P_{k+1}).\\] We could separately prove each side of this implication, but we can also use a chain of equivalences.\nBy the associativity of the “and” operator, we have \\[\\neg (P_1 \\land \\cdots \\land P_k \\land P_{k+1}) \\leftrightarrow \\neg [(P_1 \\land \\cdots \\land P_k) \\land P_{k+1}].\\] By De Morgan’s laws, we have \\[\\neg [(P_1 \\land \\cdots \\land P_k) \\land P_{k+1}] \\leftrightarrow [\\neg (P_1 \\land \\cdots \\land P_k) \\lor \\neg P_{k+1}].\\] By our assumption that \\(Q(k)\\) is true, we have \\[[\\neg (P_1 \\land \\cdots \\land P_k) \\lor \\neg P_{k+1}] \\leftrightarrow [(\\neg P_1 \\lor \\cdots \\lor \\neg P_k) \\lor \\neg P_{k+1}].\\] Finally, by the associativity of the “or” operator, we have \\[[(\\neg P_1 \\lor \\cdots \\lor \\neg P_k) \\lor \\neg P_{k+1}] \\leftrightarrow (\\neg P_1 \\lor \\cdots \\lor \\neg P_k \\lor \\neg P_{k+1}).\\] Following the chain of equivalences, we have \\[\\neg (P_1 \\land \\cdots \\land P_k \\land P_{k+1}) \\leftrightarrow (\\neg P_1 \\lor \\cdots \\lor \\neg P_k \\lor \\neg P_{k+1}),\\] establishing the induction step.\n\n\nExercise 1.8 Use a proof by induction to prove that \\(2^n \\geq n + 1\\) for every counting number \\(n = 1, 2, \\ldots\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor the base step, we must show that \\(2^1 \\geq 1 + 1\\). This holds because \\(2^1 = 2 = 1 + 1\\).\nFor the induction step, we must show that if \\(2^k \\geq k + 1\\), then \\(2^{k+1} \\geq (k + 1) + 1\\). Suppose \\(2^k \\geq k + 1\\). By definition, \\(2^{k+1} = 2 \\times 2^k\\). We have assumed that \\(2^k \\geq k + 1\\), which implies that \\(2 \\times 2^k \\geq 2 \\times (k + 1)\\). Because \\(k \\geq 0\\), we have \\(2 \\times (k + 1) \\geq (k + 1) + 1\\). Putting this all together, we have \\[2^{k+1} = 2 \\times 2^k \\geq 2 \\times (k + 1) \\geq (k + 1) + 1,\\] proving the induction step.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Formal Logic and Proofs</span>"
    ]
  },
  {
    "objectID": "logic.html#concept-review",
    "href": "logic.html#concept-review",
    "title": "1  Formal Logic and Proofs",
    "section": "1.3 Concept review",
    "text": "1.3 Concept review\n\nConceptual orderAlphabetical order\n\n\n\nProvably true\n\nA statement whose truth can be defended against all challenges, using only rules of logical inference.\n\nSentence\n\nIn formal logic, a statement that must be either true or false, and cannot be both.\n\nSentential logic\n\nA set of rules for deducing the truth of compound sentences.\n\nNegation\n\nAn operation on a single sentence that flips the truth value of the sentence, denoted \\(\\neg P\\). \\(P\\) true means \\(\\neg P\\) is false, and \\(P\\) false means \\(\\neg P\\) is true.\n\nAnd\n\nAn operation on two sentences that indicates whether both are true, denoted \\(P \\land Q\\).\n\nOr\n\nAn operation on two sentences that indicates whether at least one is true, denoted \\(P \\lor Q\\).\n\nTautology\n\nA compound sentence that is always true, regardless of the truth value of the sentences from which it is constructed. For example, \\(P \\lor \\neg P\\) (“\\(P\\) is true, or \\(P\\) is not true”) is a tautology.\n\nTruth table\n\nAn algorithm for determining the truth value of compound sentences. Each row is a unique combination of truth values of the simple sentences from which the compound is formed, and each column is a component of the compound sentence you are trying to evaluate.\n\nConditional\n\nAn if-then statement, written in formal logic as \\(P \\to Q\\), treated as the equivalent of \\(\\neg P \\lor Q\\).\n\nNecessary condition\n\n\\(Q\\) is a necessary condition for \\(P\\) when \\(Q\\) has to be true whenever \\(P\\) is true. The conditional \\(P \\to Q\\) means that \\(Q\\) is necessary for \\(P\\).\n\nSufficient condition\n\n\\(P\\) is a sufficient condition for \\(Q\\) when \\(P\\) being true guarantees that \\(Q\\) is true. The conditional \\(P \\to Q\\) means that \\(P\\) is sufficient for \\(Q\\).\n\nVacuously true\n\nA conditional statement \\(P \\to Q\\) that is true because its premise is known to be false, such as “If 0 = 1, then Peter Bils lives in a pineapple under the sea.”\n\nModus ponens\n\nThe logical rule that if \\(P\\) is true and \\(P \\to Q\\) is true, then we can infer that \\(Q\\) is true.\n\nModus tollens\n\nThe logical rule that if \\(\\neg Q\\) is true and \\(P \\to Q\\) is true, then we can infer that \\(\\neg P\\) is true.\n\nLogical equivalence\n\n\\(P\\) and \\(Q\\) are logically equivalent when \\(P \\to Q\\) and \\(Q \\to P\\). Logical equivalence holds when the truth values of the statements match — both are true or both are false.\n\nBiconditional\n\nThe statement \\(P \\leftrightarrow Q\\), meaning \\(P\\) and \\(Q\\) are logically equivalent.\n\nContrapositive\n\nThe translation of the statement \\(P \\to Q\\) into the logically equivalent statement \\(\\neg Q \\to \\neg P\\).\n\nDe Morgan’s laws\n\nConceptually, the idea that the negation of an “and” statement is an “or” statement, and vice versa. Formally, the logical equivalences \\(\\neg (P \\land Q) \\leftrightarrow \\neg P \\lor \\neg Q\\) and \\(\\neg (P \\lor Q) \\leftrightarrow \\neg P \\land \\neg Q\\).\n\nProof by contradiction\n\nA proof where we show that \\(P\\) is true by proving that \\(\\neg P\\) implies something impossible.\n\nProof by induction\n\nA proof technique used when (a) we have a claim that depends on a number \\(n\\) and (b) we want to prove it’s true for every \\(n = 1, 2, 3, \\ldots\\). In the base step, we show that the claim is true for \\(n = 1\\). In the induction step, we assume the truth of the claim for \\(n = k\\), and we show that this implies the truth of the claim for \\(n = k + 1\\).\n\n\n\n\n\nAnd\n\nAn operation on two sentences that indicates whether both are true, denoted \\(P \\land Q\\).\n\nBiconditional\n\nThe statement \\(P \\leftrightarrow Q\\), meaning \\(P\\) and \\(Q\\) are logically equivalent.\n\nConditional\n\nAn if-then statement, written in formal logic as \\(P \\to Q\\), treated as the equivalent of \\(\\neg P \\lor Q\\).\n\nContrapositive\n\nThe translation of the statement \\(P \\to Q\\) into the logically equivalent statement \\(\\neg Q \\to \\neg P\\).\n\nDe Morgan’s laws\n\nConceptually, the idea that the negation of an “and” statement is an “or” statement, and vice versa. Formally, the logical equivalences \\(\\neg (P \\land Q) \\leftrightarrow \\neg P \\lor \\neg Q\\) and \\(\\neg (P \\lor Q) \\leftrightarrow \\neg P \\land \\neg Q\\).\n\nLogical equivalence\n\n\\(P\\) and \\(Q\\) are logically equivalent when \\(P \\to Q\\) and \\(Q \\to P\\). Logical equivalence holds when the truth values of the statements match — both are true or both are false.\n\nModus ponens\n\nThe logical rule that if \\(P\\) is true and \\(P \\to Q\\) is true, then we can infer that \\(Q\\) is true.\n\nModus tollens\n\nThe logical rule that if \\(\\neg Q\\) is true and \\(P \\to Q\\) is true, then we can infer that \\(\\neg P\\) is true.\n\nNecessary condition\n\n\\(Q\\) is a necessary condition for \\(P\\) when \\(Q\\) has to be true whenever \\(P\\) is true. The conditional \\(P \\to Q\\) means that \\(Q\\) is necessary for \\(P\\).\n\nNegation\n\nAn operation on a single sentence that flips the truth value of the sentence, denoted \\(\\neg P\\). \\(P\\) true means \\(\\neg P\\) is false, and \\(P\\) false means \\(\\neg P\\) is true.\n\nOr\n\nAn operation on two sentences that indicates whether at least one is true, denoted \\(P \\lor Q\\).\n\nProof by contradiction\n\nA proof where we show that \\(P\\) is true by proving that \\(\\neg P\\) implies something impossible.\n\nProof by induction\n\nA proof technique used when (a) we have a claim that depends on a number \\(n\\) and (b) we want to prove it’s true for every \\(n = 1, 2, 3, \\ldots\\). In the base step, we show that the claim is true for \\(n = 1\\). In the induction step, we assume the truth of the claim for \\(n = k\\), and we show that this implies the truth of the claim for \\(n = k + 1\\).\n\nProvably true\n\nA statement whose truth can be defended against all challenges, using only rules of logical inference.\n\nSentence\n\nIn formal logic, a statement that must be either true or false, and cannot be both.\n\nSentential logic\n\nA set of rules for deducing the truth of compound sentences.\n\nSufficient condition\n\n\\(P\\) is a sufficient condition for \\(Q\\) when \\(P\\) being true guarantees that \\(Q\\) is true. The conditional \\(P \\to Q\\) means that \\(P\\) is sufficient for \\(Q\\).\n\nTautology\n\nA compound sentence that is always true, regardless of the truth value of the sentences from which it is constructed. For example, \\(P \\lor \\neg P\\) (“\\(P\\) is true, or \\(P\\) is not true”) is a tautology.\n\nTruth table\n\nAn algorithm for determining the truth value of compound sentences. Each row is a unique combination of truth values of the simple sentences from which the compound is formed, and each column is a component of the compound sentence you are trying to evaluate.\n\nVacuously true\n\nA conditional statement \\(P \\to Q\\) that is true because its premise is known to be false, such as “If 0 = 1, then Peter Bils lives in a pineapple under the sea.”\n\n\n\n\n\n\n\n\n\nBlack, Duncan. 1948. “On the Rationale of Group Decision-Making.” Journal of Political Economy 56 (1): 23–34.\n\n\nPrzeworski, Adam. 2024. “Who Decides What Is Democratic?” Journal of Democracy 35 (3): 5–16.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Formal Logic and Proofs</span>"
    ]
  },
  {
    "objectID": "set_theory.html",
    "href": "set_theory.html",
    "title": "2  Set Theory",
    "section": "",
    "text": "2.1 What are sets and why do we care?\nSeemingly every book on “higher math” — the kind of math you encounter after the intro calculus sequence in college — starts with a discussion of set theory. There are some deep mathematical reasons to begin this way, which you can read about on Wikipedia or, for the especially high-minded, the Stanford Encyclopedia of Philosophy. To summarize it very loosely: although most people think of math as being about numbers, most important mathematical statements (including those about numbers!) are ultimately actually rooted in the properties of sets.\nI’m putting set theory before our study of numbers-type math for reasons that are related, but more prosaic.\nAll right, so what is a set? Precisely because sets are so foundational, that turns out to be quite a sticky question to answer rigorously, so we will content ourselves with a non-rigorous definition.\nWhen we want to say briefly that \\(a\\) is an element of the set \\(A\\), we write \\(a \\in A\\). It is typical to use capital letters to denote sets and lowercase letters to denote their elements, though sometimes it is convenient or necessary to break this convention.\nAlthough sets aren’t just for numbers, there are a few sets of numbers that come up so often that we have special names for them. These are listed in Table 2.1.\nWhat if you wanted to define \\(A\\) as the set of natural numbers that are greater than 1000? Well, you could — and often should — simply write, “Let \\(A\\) be the set of natural numbers that are greater than 1000.” Don’t fall into the trap of equating rigor with fancy notation. If ordinary language can clearly communicate what you mean, use it.\nThat said, there are times when ordinary language is too cumbersome or imprecise to describe the contents of a set. Maybe there are a lot of conditions on the set you’re defining, or maybe the condition involves a finicky formula that’s hard to put into words. In these cases, it is common to use set-builder notation, for example \\[A = \\{x \\in \\mathbb{N} \\mid x &gt; 1000\\}.\\] You would read this as “\\(A\\) is the set of natural numbers \\(x\\) such that \\(x\\) is greater than 1000”.\nOr what if we wanted to define \\(B\\) as the set of values we obtain by taking a natural number and dividing it in half? We could write this in set-builder notation as \\[B = \\{x \\in \\mathbb{R} \\mid x = \\frac{n}{2} \\: \\text{for some $n \\in \\mathbb{N}$}\\}.\\] But we’ll often use the following shorthand to write this a bit less cumbersomely: \\[B = \\{\\frac{n}{2} \\mid n \\in \\mathbb{N}\\}.\\] Again, none of these, including the verbal description of \\(B\\) at the start of the paragraph, is the “right” way to denote the set. What’s best is whatever gets the point across clearly to your intended audience. Thinking about the audience is key — you might want to write the exact same mathematical object in a different way in your undergraduate lecture notes than in a manuscript you’re submitting to Political Analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Set Theory</span>"
    ]
  },
  {
    "objectID": "set_theory.html#what-are-sets-and-why-do-we-care",
    "href": "set_theory.html#what-are-sets-and-why-do-we-care",
    "title": "2  Set Theory",
    "section": "",
    "text": "Set theory gives us a lot of our mathematical “language”.\n\nMany of the definitions of day-to-day tools in calculus and linear algebra are stated in terms of sets and operations on sets.\nIn political science research, we use mathematical notation to concisely-yet-precisely summarize quantitative concepts. This notation often involves sets and operations on sets, so we need to get familiar if we want to be fluent in the “language” of math.\n\nSet theory gives us some nice examples of mathematical statements that are relatively easy to prove, making it a convenient as we get more practice writing proofs.\n\n\n\nDefinition 2.1 (Sets and elements) A set is a collection of objects, which we call the elements of the set. The notation \\[A = \\{1, 2, 4, 8\\}\\] is equivalent to saying “\\(A\\) is the set whose elements are 1, 2, 4, and 8.”\n\n\n\n\n\n\n\n\nSets aren’t just for numbers\n\n\n\nA set may consist of objects besides numbers. It is coherent to say “let \\(P\\) be the set of people who have been US presidents”, so that \\[P = \\{\\text{George Washington}, \\text{John Adams}, \\ldots, \\text{Donald Trump}\\}.\\]\n\n\n\n\n\n\nTable 2.1: Notable sets of numbers\n\n\n\n\n\n\n\n\n\n\nName\nNotation\nDefinition\n\n\n\n\nNatural numbers\n\\(\\mathbb{N}\\)\nThe counting numbers: 1, 2, 3, … and so on indefinitely. (Some textbooks treat 0 as a natural number, but usually not.)\n\n\nIntegers\n\\(\\mathbb{Z}\\)\nThe whole numbers: …, -2, -1, 0, 1, 2, …\n\n\nRational numbers\n\\(\\mathbb{Q}\\)\nNumbers that can be expressed as a quotient (hence the symbol \\(\\mathbb{Q}\\)) of integers, \\(\\frac{a}{b}\\), where \\(a \\in \\mathbb{Z}\\), \\(b \\in \\mathbb{Z}\\), and \\(b \\neq 0\\).\n\n\nReal numbers\n\\(\\mathbb{R}\\)\nEvery number on the number line, including irrational numbers like \\(\\sqrt{2}\\) and \\(\\pi\\).\n\n\nClosed interval\n\\([a, b]\\)\nEvery real number between \\(a\\) and \\(b\\), including \\(a\\) and \\(b\\) themselves.\n\n\nOpen interval\n\\((a, b)\\)\nEvery real number between \\(a\\) and \\(b\\), not including \\(a\\) and \\(b\\) themselves.\n\n\n\n\n\n\n\n\n\n\nI always use the vertical bar \\(\\mid\\) when writing in set-builder notation, but sometimes you will see people use the colon \\(:\\) instead. These are just different conventions with no differences in underlying meaning.\n\n\nExercise 2.1 Use set-builder notation to describe the following sets:\n\nEvery integer that is a multiple of 7.\nEvery natural number that is odd.\nEvery rational number between 0 and 1, including 0 and 1 themselves.\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\n\\(\\{7z \\mid z \\in \\mathbb{Z}\\}\\). (An integer that is a multiple of 7 is just some other integer multiplied by 7. For example, you can think of \\(-35\\) as \\(7 \\times -5\\).)\n\\(\\{2n - 1 \\mid n \\in \\mathbb{N}\\}\\).\n\\(\\{x \\in [0, 1] \\mid x \\in \\mathbb{Q}\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Set Theory</span>"
    ]
  },
  {
    "objectID": "set_theory.html#fundamental-operations-on-sets",
    "href": "set_theory.html#fundamental-operations-on-sets",
    "title": "2  Set Theory",
    "section": "2.2 Fundamental operations on sets",
    "text": "2.2 Fundamental operations on sets\n\n2.2.1 Subsets and equality\nWhen we’re dealing with numbers, we compare them in terms of their values. 40 is greater than 38, which in turn is greater than 10. With sets, we make comparisons in terms of their elements — namely, whether all of the elements of one are contained in another.\n\nDefinition 2.2 (Subset) The set \\(A\\) is a subset of the set \\(B\\), denoted \\(A \\subseteq B\\), if every element of \\(A\\) is an element of \\(B\\).\n\n\n\n\n\n\n\nInclusion \\(\\in\\) versus subset \\(\\subseteq\\)\n\n\n\n\\(A \\subseteq B\\) and \\(A \\in B\\) have very different meanings. \\(A \\subseteq B\\) means “every element of \\(A\\) is also an element of \\(B\\)”. \\(A \\in B\\) means “the set \\(A\\) is, itself, an element of \\(B\\)”. When you’re talking about sets, you usually mean the first of these, not the second.\nIf you mess up and use the wrong notation, your readers will probably still ultimately figure out what you mean — but it’ll take a bit longer for them than if you’d used the right notation. So it’s in your best interest to use the right notation for much the same reason as it’s in your best interest to use proper spelling and grammar.\n\n\nYou probably won’t be surprised to learn that subsets are like nesting dolls: if \\(A\\) is a subset of \\(B\\), then it is also a subset of any other set that \\(B\\) is a subset of. I’m bringing up this seemingly obvious fact mainly because it’ll give us a nice first statement about set theory to prove. Not to mention that some seemingly obvious things turn out not to be true at all — which is one of the reasons it’s important to prove our mathematical claims.\n\nProposition 2.1 (Transitivity of subset) If \\(A \\subseteq B\\) and \\(B \\subseteq C\\), then \\(A \\subseteq C\\).\n\n\nProof. We need to show that every element of \\(A\\) is also an element of \\(C\\). To do that, we will take an “arbitrary” element of \\(A\\) — a mathematical object that we know nothing about, other than that it is an element of \\(A\\) — and show that it is an element of \\(C\\). To that end, let \\(a\\) be any element of \\(A\\). Because \\(a \\in A\\) and \\(A \\subseteq B\\), it follows that \\(a \\in B\\). Because \\(a \\in B\\) and \\(B \\subseteq C\\), it follows that \\(a \\in C\\). Because the element \\(a\\) was chosen arbitrarily, we have shown that every element of \\(A\\) is an element of \\(C\\), and therefore \\(A \\subseteq C\\).\n\n\n\n\n\n\n\nProving a “for all” claim\n\n\n\nThe proof of Proposition 2.1 gets into some formal logic that we didn’t explicitly cover in Chapter 1. The proof relies on the fact that the proposition can be stated this way:\n\nPremise 1: For all \\(a \\in A\\), \\(a \\in B\\). (i.e., \\(A \\subseteq B\\))\nPremise 2: For all \\(b \\in B\\), \\(b \\in C\\). (i.e., \\(B \\subseteq C\\))\nConclusion: For all \\(a \\in A\\), \\(a \\in C\\). (i.e., \\(A \\subseteq C\\))\n\nThe method I used in the proof of Proposition 2.1 is the standard way to prove a “for all” statement. Suppose we are trying to prove a statement of the variety “for all \\(x \\in X\\), \\(P(x)\\) is true,” where \\(P(x)\\) is some sentence about \\(x\\). A proof like that typically starts with “Take an arbitrary \\(x \\in X\\),” meaning we are assuming we know nothing about \\(x\\) besides that it is a member of \\(X\\). From there, we use what we know about \\(X\\) to show that \\(P(x)\\) is indeed true.\nYou need to be very careful with negations of “for all” statements. We can falsify the statement “for all \\(x \\in X\\), \\(P(x)\\) is true” by finding a single element of \\(X\\) for which \\(P(x)\\) is false. Hence, the negation of “for all \\(x \\in X\\), \\(P(x)\\) is true” is not “for all \\(x \\in X\\), \\(P(x)\\) is false.” Instead, the correct negation is “there exists an \\(x \\in X\\) for which \\(P(x)\\) is false.”\nFYI, in case it comes up in other things you read, the symbol \\(\\forall\\) means “for all” and the symbol \\(\\exists\\) means “there exists.” I might write these on the whiteboard sometimes to save space, but I almost never use them in academic writing.\n\n\nIntuitively, two sets are equal if they have the same elements. One of the annoying-but-ultimately-good things about math is that we have to be precise about statements like “they have the same elements”. Does \\(\\{1, 2, 3\\}\\) equal \\(\\{3, 2, 1\\}\\)? Does \\(\\{a, b, c\\}\\) equal \\(\\{a, a, b, b, c, c\\}\\)? The answer to both of these questions turns out to be “yes” — but why? It comes down to the formal definition of set equality, which we state as each set being a subset of the other.\n\nDefinition 2.3 (Set equality) The sets \\(A\\) and \\(B\\) are equal, denoted \\(A = B\\), if \\(A \\subseteq B\\) and \\(B \\subseteq A\\).\n\nWe can again make an analogy to how we compare numbers. If \\(a\\) and \\(b\\) are numbers, then we have \\(a = b\\) precisely when both \\(a \\leq b\\) and \\(b \\leq a\\).\nNow let’s return to the question of whether \\(\\{1, 2, 3\\} = \\{3, 2, 1\\}\\). First we show that \\(\\{1, 2, 3\\} \\subseteq \\{3, 2, 1\\}\\):\n\n\\(1 \\in \\{3, 2, 1\\}\\) ✅\n\\(2 \\in \\{3, 2, 1\\}\\) ✅\n\\(3 \\in \\{3, 2, 1\\}\\) ✅\n\nAnd then we show that \\(\\{3, 2, 1\\} \\subseteq \\{1, 2, 3\\}\\):\n\n\\(3 \\in \\{1, 2, 3\\}\\) ✅\n\\(2 \\in \\{1, 2, 3\\}\\) ✅\n\\(1 \\in \\{1, 2, 3\\}\\) ✅\n\nYou can generalize the logic here to show that a set is the same no matter what order we write the elements in. I’ll also leave it to you to show that repetition of an element is immaterial to the membership of a set, so that (for example) \\(\\{a, b, c\\} = \\{a, a, b, b, c, c\\}\\).\nJust like equality of numbers, equality of sets is transitive: if one set is equal to another, which in turn is equal to some third set, then the first is equal to the third as well.\n\nCorollary 2.1 (Transitivity of set equality) If \\(A = B\\) and \\(B = C\\), then \\(A = C\\).\n\nBy following the template provided by the proof of Proposition 2.1, you should be able to prove this result yourself. If you’re having trouble getting started, one good principle to keep in mind when writing a proof is to take stock of all the “knowns” that might be relevant and have already been defined or proved. For this question, relevant facts would include the definition of set equality (Definition 2.3) and the transitivity of the subset operation (Proposition 2.1).\n\nExercise 2.2 Prove Corollary 2.1.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe need to show that if \\(A = B\\) and \\(B = C\\), then \\(A \\subseteq C\\) and \\(C \\subseteq A\\). By the definition of set equality, if \\(A = B\\) and \\(B = C\\), then \\(A \\subseteq B\\) and \\(B \\subseteq C\\). Proposition 2.1 then implies that \\(A \\subseteq C\\).\nAgain by the definition of set equality, if \\(C = B\\) and \\(B = A\\), then \\(C \\subseteq B\\) and \\(B \\subseteq A\\). Proposition 2.1 then implies that \\(C \\subseteq A\\).\nWe have shown that if \\(A = B\\) and \\(B = C\\), then \\(A \\subseteq C\\) and \\(C \\subseteq A\\), so by definition \\(A = C\\).\n(In the second paragraph, I implicitly relied on the commutativity of set equality, i.e., that if \\(A = B\\), then \\(B = A\\). That’s another thing you can prove yourself at this point if you’re skeptical!)\n\n\n\n\n\n\n2.2.2 The Venn diagram operations\nWe want to be able to talk about combinations of sets. Once again, a bit of mathematical notation will help us quickly convey exactly what we mean. If I say “the set of presidents and vice presidents”, do I mean all of the people who have served in either role, or only those who have served in both? We know Richard Nixon and Joe Biden are in it either way, but what about Barack Obama? Mathematically speaking, we need to be sure if we are talking about the union or the intersection of the sets of presidents and vice presidents.\n\nDefinition 2.4 (Union and intersection) The union of the sets \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the set of elements that are in \\(A\\), in \\(B\\), or both: \\[A \\cup B = \\{x \\mid x \\in A \\text{ or } x \\in B\\}.\\] The intersection of the sets \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the set of elements that are in both \\(A\\) and \\(B\\): \\[A \\cap B = \\{x \\mid x \\in A \\text{ and } x \\in B\\}.\\]\n\n\n\n\n\n\n\n\n\n\nUnion: \\(A \\cup B\\)\n\n\n\n\n\n\n\n\n\nIntersection: \\(A \\cap B\\)\n\n\n\n\n\n\n\nFigure 2.1: Union and intersection of sets.\n\n\n\n\n\nHey, the union operator \\(\\cup\\) looks a lot like the logical “or” operator \\(\\lor\\), and the intersection operator \\(\\cap\\) looks a lot like the logical “and” operator \\(\\land\\). I wonder if that’s a coincidence?\nFor example, let \\(P\\) be the set of presidents, and \\(Q\\) be the set of vice presidents. Because Barack Obama was president but not vice president, he belongs to the union of these sets, but not the intersection: \\[\\begin{gather}\n\\text{Barack Obama} \\in P \\cup Q, \\\\\n\\text{Barack Obama} \\notin P \\cap Q.\n\\end{gather}\\] Naturally, more people have been president or vice president than have been both president and vice president. In mathematical terms, the intersection \\(P \\cap Q\\) is a subset of the union \\(P \\cup Q\\). This turns out to be a general property of sets, not just of American national officeholders.\n\nProposition 2.2 \\(A \\cap B \\subseteq A \\subseteq A \\cup B\\).\n\n\nProof. There are two claims to prove here. The first is that \\(A \\cap B \\subseteq A\\). To prove this, take any element \\(x \\in A \\cap B\\). By definition of the intersection, it must be the case that \\(x \\in A\\). Therefore, \\(A \\cap B \\subseteq A\\).\nThe second claim to prove is that \\(A \\subseteq A \\cup B\\). To prove this, take any element \\(x \\in A\\). By definition of the union, \\(x \\in A \\cup B\\). Therefore, \\(A \\subseteq A \\cup B\\).\n\nWhat if we wanted to talk about the set of people who have been president, but not vice president? We can formulate this using the set difference.\n\nDefinition 2.5 (Set difference) The set difference between sets \\(A\\) and \\(B\\), denoted \\(A \\setminus B\\), is the set of elements that are in \\(A\\) and not in \\(B\\): \\[A \\setminus B = \\{a \\in A \\mid a \\notin B\\}.\\]\n\n\n\n\n\n\n\n\n\n\n\\(A \\setminus B\\)\n\n\n\n\n\n\n\n\n\n\\(B \\setminus A\\)\n\n\n\n\n\n\n\nFigure 2.2: Set differences.\n\n\n\nJoe Biden was both a vice president and a president. Barack Obama was only president, never a vice president. Aaron Burr was only vice president, never president. Therefore, we have the following set memberships:\n\n\n\n\n\n\n\n\n\n\n\n\nOld guy\n\\(P\\)\n\\(Q\\)\n\\(P \\cap Q\\)\n\\(P \\cup Q\\)\n\\(P \\setminus Q\\)\n\\(Q \\setminus P\\)\n\n\n\n\nJoe Biden\nx\nx\nx\nx\n\n\n\n\nBarack Obama\nx\n\n\nx\nx\n\n\n\nAaron Burr\n\nx\n\nx\n\nx\n\n\n\n\nExercise 2.3 Using the union, intersection, and the set difference, find a way to denote the set of elements that are in exactly one of \\(A\\) and \\(B\\), but not both.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\((A \\cup B) \\setminus (A \\cap B)\\).\n\n\n\n\nNo one who has been the president of the United States has also been the prime minister of the United Kingdom. If we were to let \\(M\\) denote the set of people who have been prime minister, then the intersection \\(P \\cap M\\) would be a set that contains … nothing? Yes, indeed, a set may contain nothing. We have a special name for the set with nothing in it — the empty set.\n\nDefinition 2.6 (Empty set) The empty set is the set containing no elements, denoted \\(\\emptyset\\).\n\nUsing this notation, a concise way to say “no one has been both the US president and the UK prime minister” would be \\(P \\cap M = \\emptyset\\). We say two sets are disjoint when their intersection is empty.\n\nExercise 2.4 Prove the following properties of the empty set:\n\n\\(A \\cup \\emptyset = A\\).\n\\(A \\cap \\emptyset = \\emptyset\\).\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nWe know from Proposition 2.2 that \\(A \\subseteq A \\cup \\emptyset\\), so we only need to prove that \\(A \\cup \\emptyset \\subseteq A\\). To this end, take any element \\(x \\in A \\cup \\emptyset\\). By definition of the union, it must be the case that \\(x \\in A\\) or that \\(x \\in \\emptyset\\). The second of these is impossible, so it must be the case that \\(x \\in A\\). As \\(x\\) was chosen arbitrarily, this proves that \\(A \\cup \\emptyset \\subseteq A\\). Finally, because \\(A \\subseteq A \\cup \\emptyset\\) and \\(A \\cup \\emptyset \\subseteq A\\), we have \\(A \\cup \\emptyset = A\\).\nWe know from Proposition 2.2 that \\(A \\cap \\emptyset \\subseteq \\emptyset\\). If there were an element \\(x \\in A \\cap \\emptyset\\), then it would follow from the definition of intersection that \\(x \\in \\emptyset\\). But that cannot be the case, as \\(\\emptyset\\) is the set with no elements. Therefore, there is no element \\(x\\) included in \\(A \\cap \\emptyset\\). By definition, then \\(A \\cap \\emptyset = \\emptyset\\).\n\n\n\n\n\n\n\n2.2.3 Complements and De Morgan’s laws\nThe set difference \\(A \\setminus B\\) is the set of everything in \\(A\\) that’s not in \\(B\\). But what if we simply wanted the set of everything that’s not in \\(B\\), whether it’s in \\(A\\) or not? Once we can properly define this type of set, we will call it the complement of \\(B\\).\nBefore we can do that, we need to be clear about what “everything” is. For example, again taking \\(P\\) to be the set of people who have been the US president, it seems clear enough that \\[\\text{Aaron Burr} \\in \\{x \\mid x \\notin P\\}.\\] But would we say that the number 3 is also an element of this set? What about the chemical formula H\\(_2\\)O or the human gene CNR1? If nothing else, it doesn’t seem to be very useful to say that these are not people who have been president, true as that may be.\nTo have a useful working definition of “everything not in this set”, we are going to assume there’s a set \\(U\\) that contains the universe of objects we might be interested in. The appropriate choice of universal set depends on the context for what we want to do. Most commonly, if we are talking about numbers that lie along the typical number line, our universal set might be the real line, \\(\\mathbb{R}\\). Or if we are talking about who has and has not held particular political positions, our universal set might be the set of all people who have ever lived.\nOnce we have settled on the universe of objects we care about, it’s simple to define the complement of a set.\n\nDefinition 2.7 (Complement) The complement of the set \\(A\\), denoted \\(A^c\\), is the set of all elements in the universe that are not in \\(A\\), i.e., \\(A^c = U \\setminus A\\).\n\n\n\n\n\n\n\n\n\n\n\\(A^c\\)\n\n\n\n\n\n\n\n\n\n\\(B^c\\)\n\n\n\n\n\n\n\nFigure 2.3: Set differences.\n\n\n\nYou don’t often see the universal set explicitly specified in mathematical writing, other than textbook set theory treatments like what you’re reading right now. I assume this is because most mathematical writers follow the same conventions I do:\n\nThe appropriate definition of a universal set should be clear from the context. For example, if all of the mathematical objects discussed are real numbers and sets of real numbers, then \\(U = \\mathbb{R}\\).\nIf the context is not clear enough for there to be a natural choice of universal set (e.g., you’re talking about sets of radically different types of objects), then use explicit set differences when needed rather than taking complements of sets.\n\nTaking our universe as the set of all people, let’s think about the complement of the union of sets of everyone who’s been a president or vice president, \\((P \\cup Q)^c\\). Anyone who’s been a president or a vice president is an element of \\(P \\cup Q\\). Therefore, if someone is not an element of \\(P \\cup Q\\), that tells us that the person has never been president and has never been vice president. In other words, that person belongs to both \\(P^c\\) and \\(Q^c\\), and thus also to their intersection \\(P^c \\cap Q^c\\). This is an illustration of a very useful pair of set properties called De Morgan’s laws.\n\nTheorem 2.1 (De Morgan’s laws)  \n\n\\((A \\cup B)^c = A^c \\cap B^c\\).\n\\((A \\cap B)^c = A^c \\cup B^c\\).\n\n\n\n\n\n\n\n\n\n\n\n\\((A \\cup B)^c = A^c \\cap B^c\\)\n\n\n\n\n\n\n\n\n\n\\((A \\cap B)^c = A^c \\cup B^c\\)\n\n\n\n\n\n\n\nFigure 2.4: De Morgan’s laws.\n\n\n\n\n\nHey, these look a lot like the De Morgan’s laws for formal logic (Theorem 1.3)! I wonder if that’s a coincidence?\n\nProof. I’ll prove the first law, and leave the second to you as Exercise 2.5. First we have to prove that \\((A \\cup B)^c \\subseteq A^c \\cap B^c\\). To this end, take any \\(x \\in (A \\cup B)^c\\), so that \\(x \\notin A \\cup B\\). By definition of the union, this implies \\(x \\notin A\\), hence \\(x \\in A^c\\). By the same token, \\(x \\notin B\\), hence \\(x \\in B^c\\). Because \\(x \\in A^c\\) and \\(x \\in B^c\\), we have \\(x \\in A^c \\cap B^c\\). As \\(x\\) was chosen arbitrarily, this shows that \\((A \\cup B)^c \\subseteq A^c \\cap B^c\\).\nNext, to complete the proof of equality, we have to prove that \\(A^c \\cap B^c \\subseteq (A \\cup B)^c\\). To this end, take any \\(x \\in A^c \\cap B^c\\). By definition of the intersection, \\(x \\in A^c\\), hence \\(x \\notin A\\). By the same token, \\(x \\in B^c\\), hence \\(x \\notin B\\). Because \\(x \\notin A\\) and \\(x \\notin B\\), we have \\(x \\notin A \\cup B\\) and thus \\(x \\in (A \\cup B)^c\\). As \\(x\\) was chosen arbitrarily, this shows that \\(A^c \\cap B^c \\subseteq (A \\cup B)^c\\).\n\n\nExercise 2.5 Prove that \\((A \\cap B)^c = A^c \\cup B^c\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe steps are the same as in the proof of the first part above, so I’ll be a bit briefer about them.\nTake any \\(x \\in (A \\cap B)^c\\). Because \\(x \\notin A \\cap B\\), it must be the case that \\(x \\in A^c\\) or \\(x \\in B^c\\). If \\(x \\in A^c\\), then \\(x \\in A^c \\cup B^c\\). Otherwise, if \\(x \\notin A^c\\), then \\(x \\in B^c\\) and thus \\(x \\in A^c \\cup B^c\\). Either way, we have \\(x \\in A^c \\cup B^c\\). Consequently, \\((A \\cap B)^c \\subseteq A^c \\cup B^c\\).\nNow take any \\(x \\in A^c \\cup B^c\\). It must be the case that \\(x \\in A^c\\) or \\(x \\in B^c\\). If \\(x \\in A^c\\), then \\(x \\notin A\\), hence \\(x \\notin A \\cap B\\), hence \\(x \\in (A \\cap B)^c\\). Otherwise, if \\(x \\notin A^c\\), then \\(x \\in B^c\\), hence \\(x \\notin B\\), hence \\(x \\notin A \\cap B\\), hence \\(x \\in (A \\cap B)^c\\). Either way, we have \\(x \\in (A \\cap B)^c\\). Consequently, \\(A^c \\cup B^c \\subseteq (A \\cap B)^c\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Set Theory</span>"
    ]
  },
  {
    "objectID": "set_theory.html#functions",
    "href": "set_theory.html#functions",
    "title": "2  Set Theory",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nEvery US president took office at some recognizable point in time. George Washington’s first term began in 1789. Grover Cleveland’s first term began in 1893. Barack Obama’s first term began in 2009. I could go on, but you get the point. For every president, there is exactly one year when they became president for the first time.\nWhat we’ve done here is associate each president, an element of set \\(P\\) as we’ve been calling it, with a natural number, an element of \\(\\mathbb{N}\\). “The year that a given president took office” is what we call a function, which we might say “maps” one set into another. The notation \\(f : A \\to B\\) is a bit of mathematical shorthand that means “\\(f\\) is a rule that takes each element of \\(A\\) and associates it with an element of \\(B\\)”. For any element \\(a \\in A\\), we write \\(f(a)\\) to stand for the element of \\(B\\) that the function associates \\(a\\) with. When a function maps \\(A\\) into \\(B\\), we call \\(A\\) the domain and \\(B\\) the codomain.\nTo continue the example, let \\(y : P \\to \\mathbb{N}\\) (verbally: “\\(y\\) is a function that maps the set of presidents into the set of natural numbers”) be the function that associates each president with the year they first took office. Then we have \\(y(\\text{George Washington}) = 1789\\), \\(y(\\text{Grover Cleveland}) = 1893\\), and so on. The domain of this function is the set of presidents, \\(P\\), and the codomain is the set of natural numbers, \\(\\mathbb{N}\\).\nSome associations between sets are not functions. When we’re thinking about the set of presidents, \\(P\\), you might be tempted to think about a function that associates each president with their vice president. However, no such function exists, for two reasons.\n\nA function associates every element of its domain with an element of the codomain. However, some presidents had no vice president. For example, Andrew Johnson, who assumed the presidency after Abraham Lincoln’s assassination, never had a vice president.\nA function associates each element of the domain with exactly one element of the codomain. However, some presidents have had multiple vice presidents. For example, both Spiro Agnew and Gerald Ford served as vice president to Richard Nixon.\n\nIn mathematical terms, we would call the president-and-their-vice-president(s-if-any) assocation a relation. Any function is a relation, but many relations are not functions. There are lots of cool things to study about relations, but I cannot say in good faith that you must know these cool things, so that is all I will say about relations.\n\nExercise 2.6 Come up with a function that maps the set of presidents, \\(P\\), into the set of vice presidents, \\(Q\\). Convince yourself that it satisfies the key requirement of a function — that every element in \\(P\\) is associated with one, and exactly one, element in \\(Q\\).\n\n\n\n\n\n\nOne answer\n\n\n\n\n\nI propose the function \\(f : P \\to Q\\) defined so that \\(f(p) = \\text{Joe Biden}\\) for every president \\(p \\in P\\). This rule associates every president with a vice president, so it is a function.\nThis might seem like a dumb example, but it meets the criteria that I set out, making it a valid answer. Math is legalistic in this way: the question asked for a function \\(f : P \\to Q\\), I provided a function \\(f : P \\to Q\\), and therefore I got the answer right. So if you’re here in a political science PhD program because your law school dreams didn’t quite work out, take comfort in knowing that you can still get to be a legalistic jackass when you’re working in the world of math.\n\n\n\n\n\n2.3.1 One-to-one and onto functions\nAs you saw in Exercise 2.6, there is nothing in the definition of a function to suggest that every element of the domain must be associated with a different element of the codomain. Some functions do have this additional property, and we call those functions one-to-one.\n\nDefinition 2.8 (One-to-one) A function \\(f : A \\to B\\) is one-to-one, also called injective, if \\(f(a) \\neq f(a')\\) for all distinct elements \\(a \\in A\\) and \\(a' \\in A\\).\n\n\n\n\n\n\n\n\n\nFigure 2.5: A function that is one-to-one, but not onto.\n\n\n\n\n\nThe function \\(y : P \\to \\mathbb{N}\\) that we discussed earlier, mapping presidents into the year that they first took office, is not one-to-one. William Henry Harrison took office in March 1841, famously gave a lengthy speech in poor weather, caught cold, and died a month later. His vice president, John Tyler, took office in April 1841. In terms of the function we defined, \\[y(\\text{William Henry Harrison}) = y(\\text{John Tyler}) = 1841.\\] Because these are two different presidents with the same function value, \\(y\\) is not one-to-one.\nYou also saw in Exercise 2.6 that a function need not reach every element of its codomain. In fact, if the domain has fewer elements than the codomain — such as with the sets of presidents \\(P\\) and vice presidents \\(Q\\), where at the time of writing there have been 45 presidents and 50 vice presidents — it would be impossible for the function to reach every element of the codomain. In the special case where a function does reach every element of the codomain, we turn a preposition into an adjective and say the function is onto.\n\nDefinition 2.9 (Onto) A function \\(f : A \\to B\\) is onto, also called surjective, if for every element \\(b \\in B\\) there is some element \\(a \\in A\\) such that \\(f(a) = b\\).\n\n\n\n\n\n\n\n\n\nFigure 2.6: A function that is onto, but not one-to-one.\n\n\n\n\n\nWe have already seen that the function that maps presidents into the first year they took office is not one-to-one. It is not onto either. Consider 1999, the year The Matrix came out and inspired legions of nerds (like me…) to make green-on-black their default color theme for computing. 1999 is a natural number, or \\(1999 \\in \\mathbb{N}\\) if you want to be formal about it, and yet no president took office for the first time then; Bill Clinton, who had first taken office in 1993, was the president the entire year. We have found a number \\(n \\in \\mathbb{N}\\) such that \\(y(p) \\neq n\\) for all presidents \\(p \\in P\\), meaning \\(y\\) is not onto.\nFunctions that are both one-to-one and onto, which we call bijections, are special. What makes them special is that they can be reversed, or inverted in the language of mathematics. I like to think of a bijection as creating a “buddy system” between its domain and a codomain. Think about a bijection \\(f\\) that maps elements of (domain) \\(A\\) into (codomain) \\(B\\), so we’d write \\(f : A \\to B\\). Then for every element \\(a\\) of \\(A\\), there is exactly one element \\(b\\) of \\(B\\) such that \\(f(a) = b\\). And for every element \\(b\\) of \\(B\\), there is exactly one element \\(a\\) of \\(A\\) such that \\(f(a) = b\\). If the function weren’t one-to-one, then there’d be at least one \\(a \\in A\\) with multiple matches in \\(B\\). If it weren’t onto, then there’d be at least one \\(b \\in B\\) with no matches in \\(A\\).\n\nDefinition 2.10 (Bijection and inverse) A function \\(f : A \\to B\\) is a bijection if it is both one-to-one and onto.\nEvery bijective function has an inverse function, \\(f^{-1} : B \\to A\\). For every pair of elements \\(a \\in A\\) and \\(b \\in B\\), we have \\(f^{-1}(b) = a\\) if and only if \\(f(a) = b\\).\n\n\n\n\n\n\n\n\n\n\nA bijection…\n\n\n\n\n\n\n\n\n\n\n\n…and its inverse.\n\n\n\n\n\n\n\nFigure 2.7: A function that is both one-to-one and onto.\n\n\n\nFigure 2.7 illustrates the buddy system property of a bijection. Each element of \\(A\\) has exactly one buddy in \\(B\\), and each element of \\(B\\) has exactly one buddy in \\(A\\). For finite sets like the ones in the illustration, you can probably convince yourself that they’d have to have the same number of elements in order for this kind of buddy system to be viable. All I’ll say here for infinite sets is that it gets weirder; read the optional Section 2.3.2 below if you want to peek into the weirdness.\nWe’ll conclude our main discussion of functions with one more proof. If you look at Figure 2.7, you’ll see that the inverse of \\(f\\) is one-to-one and onto. This is not just an accidental feature of the illustration; it’s a general feature of the inverse function of a bijection. If you’re not convinced, here’s a proof. (And if you are convinced—shouldn’t you have asked for proof instead of just taking my word?)\n\nProposition 2.3 (The inverse of a bijection is a bijection) If \\(f\\) is a bijection, then its inverse \\(f^{-1}\\) is a bijection.\n\n\nProof. Assume \\(f : A \\to B\\) is a bijection. To prove the proposition, we need to show that \\(f^{-1} : B \\to A\\) is both one-to-one and onto.\nTo show that \\(f^{-1}\\) is one-to-one, we need to show that \\(f^{-1}(b) \\neq f^{-1}(b')\\) for any distinct elements \\(b \\in B\\) and \\(b' \\in B\\). To accomplish this, take any pair of distinct \\(b \\in B\\) and \\(b' \\in B\\). (By distinct, we mean that \\(b \\neq b'\\).) Because \\(f\\) is onto, there is an \\(a \\in A\\) such that \\(f(a) = b\\) and an \\(a' \\in A\\) such that \\(f(a') = b'\\). As \\(b\\) and \\(b'\\) are distinct, we must have \\(a \\neq a'\\); otherwise, we would have \\(f(a) = f(a')\\) and thus \\(b = b'\\). Meanwhile, by definition of the inverse function, \\(f^{-1}(b) = a\\) and \\(f^{-1}(b') = a'\\). Putting this together with the fact that \\(a \\neq a'\\), we have \\(f^{-1}(b) \\neq f^{-1}(b')\\). As \\(b\\) and \\(b'\\) were chosen arbitrarily, this proves that \\(f^{-1}(b) \\neq f^{-1}(b')\\) for any distinct \\(b \\in B\\) and \\(b' \\in B\\), which means \\(f^{-1}\\) is one-to-one.\nTo show that \\(f^{-1}\\) is onto, we need to show that for every element \\(a \\in A\\), there is an element \\(b \\in B\\) such that \\(f^{-1}(b) = a\\). To accomplish this, take any element \\(a \\in A\\). There is an element \\(b \\in B\\) such that \\(f(a) = b\\). By definition of the inverse function, \\(f^{-1}(b) = a\\). As \\(a\\) was chosen arbitrarily, this proves that for every \\(a \\in A\\) there is some \\(b \\in B\\) such that \\(f^{-1}(b) = a\\), which means \\(f^{-1}\\) is onto.\n\n\n\n2.3.2 An optional digression into cardinality and infinities\nThis section contains material that is not strictly necessary, but which I find edifying and hope you will too.\nThe cardinality of a set, loosely speaking, is the number of elements in the set. We write \\(|A|\\) to denote the cardinality of a set \\(A\\). For a finite set, this loose definition is exact — the cardinality of a finite set is simply its size. For example, if \\(A = \\{1.1, 2.2, 4.4\\}\\), then \\(|A| = 3\\).\nIt gets more complicated once we start dealing with infinite sets. You might think that \\(|A| = \\infty\\) for infinite sets. But you would be wrong, because it turns out some infinities are bigger than others. The goal of this section is to show you why.\nFirst we need to define what it means for two sets to have the same cardinality. With finite sets, this is simple — they have the same number of elements. To extend this to infinite sets, we will rely on bijections. We will say that two sets have the same cardinality if we can set up a buddy system between the two of them.\n\nDefinition 2.11 (Equal cardinality) The sets \\(A\\) and \\(B\\) have the same cardinality, denoted \\(|A| = |B|\\), if there is a bijective function that maps \\(A\\) into \\(B\\).\n\nIt is obvious enough that this definition “works” for finite sets. Consider the sets \\(A = \\{\\text{Coke}, \\text{Pepsi}, \\text{Mountain Dew}\\}\\) and \\(B = \\{10, 17, 27\\}\\). We know just from looking at these sets that \\(|A| = |B| = 3\\). To prove that the formal definition is satisfied, it’s easy enough to produce a bijection between them, such as the function \\(f : A \\to B\\) defined by \\[\n\\begin{aligned}\nf(\\text{Coke}) &= 10, \\\\\nf(\\text{Pepsi}) &= 17, \\\\\nf(\\text{Mountain Dew}) &= 27.\n\\end{aligned}\n\\]\nIn a way, it is easier to prove that two sets have equal cardinality than to prove that they don’t. To prove that \\(|A| = |B|\\), we just need to find one bijection between them. But to prove that \\(|A| \\neq |B|\\), we need to show that every function between them is not a bijection.\n\nExercise 2.7 Consider the sets \\[\n\\begin{aligned}\nA &= \\{\\text{Coke}, \\text{Pepsi}, \\text{Mountain Dew}\\}, \\\\\nB &= \\{10, 17\\}, \\\\\nC &= \\{10, 17, 27, 42\\}.\n\\end{aligned}\n\\] Using the formal definition of equal cardinality, prove that \\(|A| \\neq |B|\\) and \\(|A| \\neq |C|\\).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo prove the first claim, we must show that there is no bijection mapping \\(A\\) into \\(B\\) — or vice versa (remember from Proposition 2.3 that if there is a bijection from \\(A\\) into \\(B\\), then there is also a bijection from \\(B\\) into \\(A\\)). We can easily show that there is no onto function mapping \\(B\\) into \\(A\\). Consider any function \\(f : B \\to A\\) such that \\(f(b) = \\text{Coke}\\) for some \\(b \\in B\\) and \\(f(b') = \\text{Pepsi}\\) for some \\(b' \\in B\\). Then we must have either \\(b = 10\\) and \\(b' = 17\\), or else \\(b = 17\\) and \\(b' = 10\\). Either way, there is no \\(b'' \\in B\\) such that \\(f(b'') = \\text{Mountain Dew}\\). Therefore, there is no onto function, and thus no bijection, mapping \\(B\\) into \\(A\\). We conclude that \\(|A| \\neq |B|\\).\nThe proof that \\(|A| \\neq |C|\\) follows almost the same steps. You just need to show that there is no onto function mapping \\(A\\) into \\(C\\).\n\n\n\n\nOur definition of equal cardinality operates totally intuitively with finite sets. It is not so intuitive with infinite sets. Let \\(\\mathbb{N}_E\\) stand for the set of even natural numbers, so that \\[\\mathbb{N}_E = \\{2n \\mid n \\in \\mathbb{N}\\} = \\{2, 4, 6, \\ldots\\}.\\] It sure looks like \\(\\mathbb{N}_E\\) is “smaller” than \\(\\mathbb{N}\\). After all, every even natural number is a natural number, yet not every natural number is an even natural number (for example, 1). In mathematical notation, \\(\\mathbb{N}_E \\subseteq \\mathbb{N}\\) and \\(\\mathbb{N} \\nsubseteq \\mathbb{N}_E\\). Nonetheless, these two sets turn out to have the same cardinality.\n\nExample 2.1 (Naturals and even naturals have same cardinality) Consider the function \\(f : \\mathbb{N} \\to \\mathbb{N}_E\\) defined by \\(f(n) = 2n\\). To see that \\(f\\) is one-to-one, observe that if \\(n \\neq n'\\), then \\(f(n) = 2n \\neq 2n' = f(n')\\). To see that \\(f\\) is onto, take any even natural number \\(e \\in \\mathbb{N}_E\\). By definition of an even natural number, there exists a natural number \\(n\\) such that \\(2n = e\\). Therefore, \\(f(n) = e\\). As \\(e\\) was chosen arbitrarily, this means that for every \\(e \\in \\mathbb{N}_E\\) there is an \\(n \\in \\mathbb{N}\\) such that \\(f(n) = e\\), and thus \\(f\\) is onto. We have shown that \\(f\\) is a bijection, so \\(|\\mathbb{N}| = |\\mathbb{N}_E|\\).\n\n\n\n\n\n\nThis result is one hint at the weirdness of infinite sets. A more vivid way to describe the weirdness is the metaphor of Hilbert’s Hotel. Imagine a hotel with infinitely many rooms, one for each natural number. A traveler arrives at the hotel, only to find out every room is full. She starts to walk out the door, when the manager tells her, “Don’t worry, we can make room.” He tells the occupant of room 1 to move to room 2, whose occupant goes to room 3, whose occupant goes to room 4, and so on, with the guest of each room \\(n\\) being moved to room \\(n + 1\\). The new arrival can now move into room 1, even though the hotel was full when she arrived and no one has checked out.\nIf you buy that Hilbert’s Hotel can accommodate one additional guest even when it’s full, then you will probably agree that it could take any finite number of new guests. If \\(m\\) guests arrive, have the guest in each room \\(n\\) move to room \\(n + m\\). What is more surprising is that Hilbert’s Hotel can also accommodate an infinite number of new guests. The logic is an application of Example 2.1. For each current guest in room \\(n\\), have them move to room \\(2n - 1\\). Now all of the even rooms are empty, and we can put the first new guest in our infinite sequence of new arrivals into room 2, the second into room 4, and so on. This seems crazy. It is crazy. Infinite quantities behave in strange ways, and we will go astray if we try to apply the rules of ordinary numbers to infinities.\n\n\nWe can make it even crazier if we want. Suppose an infinite number of trains arrive, each of which contains an infinite number of guests. Because the set of all pairs of natural numbers has the same cardinality as the set of natural numbers, it turns out that the ostensibly full hotel can accommodate this infinity-upon-infinity of new guests too.\nWe’ve seen that there are as many natural numbers as there are even natural numbers. This result might lead you to think that all infinite sets have the same cardinality. In fact, that’s not the case. Some infinities are bigger than others. For example, the cardinality of the real numbers is greater than the cardinality of the natural numbers.\n\n\nIs there any set whose cardinality is in between these two? Mathematicians literally, and famously, cannot decide.\nTo see why there are more real numbers than natural numbers, we will sketch out the diagonal argument from the 19th century mathematician Georg Cantor. (It’s a sketch because there are some nagging details that we will ignore. For example, we are ignoring the possibility of two different decimal expansions corresponding to the same number, as is the case with 0.1 and 0.099999\\(\\cdots\\).) Remember that two sets have equal cardinality if there is a bijection that maps one into the other. So to prove that the cardinalities of the natural numbers \\(\\mathbb{N}\\) and the real numbers \\(\\mathbb{R}\\) are different, we will show that no function \\(f : \\mathbb{N} \\to (0, 1)\\) can be onto, and thus no bijection meeting the definition of equal cardinality may exist.\nTake any function \\(f : \\mathbb{N} \\to (0, 1)\\). This function associates each natural number \\(n = 1, 2, \\ldots\\) with a real number between 0 and 1 (not inclusive). We want to show that there is a real number \\(x \\in [0, 1]\\) outside the range of \\(f\\), i.e., \\(f(n) \\neq x\\) for all \\(n \\in \\mathbb{N}\\). To see how we’re going to do this, let’s imagine lining up each \\(f(n)\\) and taking their decimal expansion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n\\)\n\\(f(n)\\)\ndigit 1\ndigit 2\ndigit 3\ndigit 4\ndigit 5\n…\n\n\n\n\n1\n0.04052…\n0\n4\n0\n5\n2\n…\n\n\n2\n0.65077…\n6\n5\n0\n7\n7\n…\n\n\n3\n0.97986…\n9\n7\n9\n8\n6\n…\n\n\n4\n0.57433…\n5\n7\n4\n3\n3\n…\n\n\n5\n0.09802…\n0\n9\n8\n0\n2\n…\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\nWe want to find an \\(x\\) that is not equal to any \\(f(n)\\). We can construct this by ensuring that the \\(n\\)’th decimal of \\(x\\) differs from the \\(n\\)’th decimal of each \\(f(n)\\).\n\n\n\n\\(n\\)\n\\(f(n)\\)\n\\(n\\)’th digit of \\(f(n)\\)\n\\(n\\)’th digit of \\(x\\)\n\n\n\n\n1\n0.04052…\n0\n1\n\n\n2\n0.65077…\n5\n6\n\n\n3\n0.97986…\n9\n0\n\n\n4\n0.57433…\n3\n4\n\n\n5\n0.09802…\n2\n3\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nBy constructing \\(x\\) this way, we ensure that there is no \\(n\\) where \\(f(n)\\) equals \\(x\\), because we know that it differs in at least one decimal place from every single \\(f(n)\\). This means there is no onto function that maps from the natural numbers into \\((0, 1)\\), and therefore no onto function that maps from the natural numbers into the real numbers, and therefore these sets have different cardinalities. In other words, there is more than one infinity!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Set Theory</span>"
    ]
  },
  {
    "objectID": "set_theory.html#concept-review",
    "href": "set_theory.html#concept-review",
    "title": "2  Set Theory",
    "section": "2.4 Concept review",
    "text": "2.4 Concept review\n\nConceptual orderAlphabetical order\n\n\n\nSet theory\n\nThe study of sets and their properties. A key logical foundation for much of the rest of mathematics.\n\nSet\n\nInformally, a collection of objects (not necessarily numbers). A set \\(S\\) with elements \\(a\\), \\(b\\), and \\(c\\) is written \\(S = \\{a, b, c\\}\\).\n\nElement\n\nA member of a set. We write \\(a \\in A\\) to mean “\\(a\\) is an element of the set \\(A\\).”\n\nSet-builder notation\n\nA way to describe a set without explicitly enumerating all of its elements. For example, \\(\\{a \\in A \\mid a &gt; 3\\}\\) means “The set of all elements of \\(A\\) that are greater than 3.\n\nSubset\n\nWhen all of the elements of the set \\(A\\) are also elements of the set \\(B\\), we say that \\(A\\) is a subset of \\(B\\), written \\(A \\subseteq B\\).\n\nUnion\n\nThe union of the sets \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the set of all elements that are in at least one of \\(A\\) or \\(B\\).\n\nIntersection\n\nThe intersection of the sets \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the set of all elements that are in both \\(A\\) and \\(B\\).\n\nSet difference\n\nThe set difference between \\(A\\) and \\(B\\), denoted \\(A \\setminus B\\), is the set of all elements that are in \\(A\\) and are not in \\(B\\).\n\nEmpty set\n\nThe set containing no elements, denoted \\(\\emptyset\\).\n\nDisjoint sets\n\nTwo sets are disjoint if they have no elements in common, or equivalently if their intersection is the empty set.\n\nUniverse\n\nThe set of everything that could conceivably be in the sets we’re talking about. The relevant universe is highly dependent on the specific context, and ought to be inferable from context clues if not explicitly stated.\n\nComplement\n\nThe complement of a set \\(A\\), denoted \\(A^c\\), is the set of all elements in the relevant universe that are not elements of \\(A\\).\n\nDe Morgan’s laws\n\nRules for taking the complement of a union or an intersection of sets. The complement of a union is the intersection of the individual complements, and the complement of an intersection is the union of the individual complements.\n\nFunction\n\nA rule that associates each element in one set (the domain) with a single element in another set (the codomain). We write \\(f : A \\to B\\) to denote a function \\(f\\) with domain \\(A\\) and codomain \\(B\\).\n\nDomain\n\nThe set that a function “acts on”. If \\(f\\) is a function whose domain is \\(A\\), then \\(f(a)\\) is defined for every element \\(a \\in A\\).\n\nCodomain\n\nThe set that a function “maps into”. If \\(f\\) is a function whose domain is \\(A\\) and whose codomain is \\(B\\), then \\(f(a)\\) is an element of \\(B\\) for every element \\(a \\in A\\).\n\nOne-to-one\n\nA function is one-to-one if it associates each element of the domain with a distinct element of the codomain: if \\(a \\neq a'\\), then \\(f(a) \\neq f(a')\\).\n\nInjective\n\nAnother name for one-to-one.\n\nOnto\n\nA function is onto if it maps at least one element of the domain to every element of the codomain: for all \\(b\\) in the codomain, there is a domain element \\(a\\) such that \\(f(a) = b\\).\n\nSurjective\n\nAnother name for onto.\n\nBijection\n\nA function that is both one-to-one (injective) and onto (surjective).\n\nInverse function\n\nThe reverse of a function, denoted \\(f^{-1}\\), where \\(f^{-1}(b) = a\\) if and only if \\(f(a) = b\\). Only exists for functions that are bijective (one-to-one and onto).\n\nCardinality\n\nThe cardinality of a set \\(A\\), denoted \\(|A|\\), is the number of elements in the set if it is finite. It gets more complicated if the set is infinite.\n\n\n\n\n\nBijection\n\nA function that is both one-to-one (injective) and onto (surjective).\n\nCardinality\n\nThe cardinality of a set \\(A\\), denoted \\(|A|\\), is the number of elements in the set if it is finite. It gets more complicated if the set is infinite.\n\nCodomain\n\nThe set that a function “maps into”. If \\(f\\) is a function whose domain is \\(A\\) and whose codomain is \\(B\\), then \\(f(a)\\) is an element of \\(B\\) for every element \\(a \\in A\\).\n\nComplement\n\nThe complement of a set \\(A\\), denoted \\(A^c\\), is the set of all elements in the relevant universe that are not elements of \\(A\\).\n\nDe Morgan’s laws\n\nRules for taking the complement of a union or an intersection of sets. The complement of a union is the intersection of the individual complements, and the complement of an intersection is the union of the individual complements.\n\nDisjoint sets\n\nTwo sets are disjoint if they have no elements in common, or equivalently if their intersection is the empty set.\n\nDomain\n\nThe set that a function “acts on”. If \\(f\\) is a function whose domain is \\(A\\), then \\(f(a)\\) is defined for every element \\(a \\in A\\).\n\nElement\n\nA member of a set. We write \\(a \\in A\\) to mean “\\(a\\) is an element of the set \\(A\\).”\n\nEmpty set\n\nThe set containing no elements, denoted \\(\\emptyset\\).\n\nFunction\n\nA rule that associates each element in one set (the domain) with a single element in another set (the codomain). We write \\(f : A \\to B\\) to denote a function \\(f\\) with domain \\(A\\) and codomain \\(B\\).\n\nInjective\n\nAnother name for one-to-one.\n\nIntersection\n\nThe intersection of the sets \\(A\\) and \\(B\\), denoted \\(A \\cap B\\), is the set of all elements that are in both \\(A\\) and \\(B\\).\n\nInverse function\n\nThe reverse of a function, denoted \\(f^{-1}\\), where \\(f^{-1}(b) = a\\) if and only if \\(f(a) = b\\). Only exists for functions that are bijective (one-to-one and onto).\n\nOne-to-one\n\nA function is one-to-one if it associates each element of the domain with a distinct element of the codomain: if \\(a \\neq a'\\), then \\(f(a) \\neq f(a')\\).\n\nOnto\n\nA function is onto if it maps at least one element of the domain to every element of the codomain: for all \\(b\\) in the codomain, there is a domain element \\(a\\) such that \\(f(a) = b\\).\n\nSet\n\nInformally, a collection of objects (not necessarily numbers). A set \\(S\\) with elements \\(a\\), \\(b\\), and \\(c\\) is written \\(S = \\{a, b, c\\}\\).\n\nSet difference\n\nThe set difference between \\(A\\) and \\(B\\), denoted \\(A \\setminus B\\), is the set of all elements that are in \\(A\\) and are not in \\(B\\).\n\nSet theory\n\nThe study of sets and their properties. A key logical foundation for much of the rest of mathematics.\n\nSet-builder notation\n\nA way to describe a set without explicitly enumerating all of its elements. For example, \\(\\{a \\in A \\mid a &gt; 3\\}\\) means “The set of all elements of \\(A\\) that are greater than 3.\n\nSubset\n\nWhen all of the elements of the set \\(A\\) are also elements of the set \\(B\\), we say that \\(A\\) is a subset of \\(B\\), written \\(A \\subseteq B\\).\n\nSurjective\n\nAnother name for onto.\n\nUnion\n\nThe union of the sets \\(A\\) and \\(B\\), denoted \\(A \\cup B\\), is the set of all elements that are in at least one of \\(A\\) or \\(B\\).\n\nUniverse\n\nThe set of everything that could conceivably be in the sets we’re talking about. The relevant universe is highly dependent on the specific context, and ought to be inferable from context clues if not explicitly stated.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Set Theory</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Black, Duncan. 1948. “On the Rationale of Group\nDecision-Making.” Journal of Political Economy 56 (1):\n23–34.\n\n\nPrzeworski, Adam. 2024. “Who Decides What Is Democratic?”\nJournal of Democracy 35 (3): 5–16.",
    "crumbs": [
      "References"
    ]
  }
]